{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from model.bAbI_utils_loader import bAbIDataset, bAbIDataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, encoding_method='basic', temporal=True, \\\n",
    "                 use_cuda=False, max_story_len=None):\n",
    "        \"\"\"\n",
    "        https://arxiv.org/pdf/1503.08895.pdf\n",
    "\n",
    "        ------------------------------------\n",
    "        vocab_size: [int], vocaburary size\n",
    "        embed_size: [int], embedding size\n",
    "        n_hops: [int], multiple computational steps\n",
    "        encoding_method: [string], \"basic\" or \"pe\", \"pe\" means position encoding\n",
    "        temporal: [boolean], Use temporal encoding method\n",
    "        use_cuda: [boolean], GPU option\n",
    "        max_story_len: [int], max story length in total data set, it is used for determining embedding size\n",
    "                       of temporal encoding. Find it at \"bAbIDataset\" class, self.max_story_len\n",
    "        \"\"\"\n",
    "        super(E2EMN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # sharing matrix for k hops & and init to normal dist.\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # TE: temporal encoding\n",
    "        if self.te:\n",
    "            assert max_story_len is not None, \\\n",
    "                'must have a fixed story_len, insert \"max_story_len\" as a number, find it at \"bAbIDataset\" class'\n",
    "            assert isinstance(max_story_len, int), '\"max_story_len\" must be a integer'\n",
    "\n",
    "            self.embed_A_T = nn.Embedding(max_story_len + 1, self.embed_size, padding_idx=0)\n",
    "            self.embed_C_T = nn.Embedding(max_story_len + 1, self.embed_size, padding_idx=0)\n",
    "            if self.use_cuda:\n",
    "                self.embed_A_T = self.embed_A_T.cuda()\n",
    "                self.embed_C_T = self.embed_C_T.cuda()\n",
    "\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self._weight_init()\n",
    "\n",
    "    def _weight_init(self):\n",
    "        for x in [self.embed_A, self.embed_B, self.embed_C]:\n",
    "            nn.init.normal_(x.weight, mean=0, std=0.1)\n",
    "        if self.te:\n",
    "            for x in [self.embed_A_T, self.embed_C_T]:\n",
    "                nn.init.normal_(x.weight, mean=0, std=0.1)\n",
    "\n",
    "    def _temporal_encoding_requirements(self, stories_masks):\n",
    "        # temporal encoding\n",
    "        if self.te:\n",
    "            story_len = stories_masks.size(1)\n",
    "            temp = stories_masks.eq(0).sum(2)  # B, n : byte tensor\n",
    "            te_idx_matrix = torch.arange(1, story_len + 1).repeat(temp.size(0)).view(temp.size()).long()\n",
    "            if self.use_cuda:\n",
    "                te_idx_matrix = te_idx_matrix.cuda()\n",
    "            te_idx_matrix = te_idx_matrix * temp.ge(1).long()  # B, n\n",
    "        else:\n",
    "            te_idx_matrix = None\n",
    "\n",
    "        return te_idx_matrix\n",
    "\n",
    "    def _pe_requirements(self, stories_masks):\n",
    "        # position encoding\n",
    "        if stories_masks is not None:\n",
    "            pe_word_lengths = stories_masks.eq(0).sum(2)  # B, n : byte tensor\n",
    "        else:\n",
    "            pe_word_lengths = None\n",
    "        return pe_word_lengths\n",
    "\n",
    "    def encoding2memory(self, embeded_x, word_length=None):\n",
    "        \"\"\"\n",
    "        embed_x: n, T_c, d\n",
    "        word_length: n\n",
    "        \"\"\"\n",
    "        if self.encoding_method == 'basic':\n",
    "            return embeded_x.sum(1)  # n, d\n",
    "\n",
    "        elif self.encoding_method == 'pe':\n",
    "            assert word_length is not None, 'insert stories_masks when forward'\n",
    "\n",
    "            T_c, d = embeded_x.size()[1:]\n",
    "            j = torch.arange(1, T_c + 1).unsqueeze(1).repeat(1, d)\n",
    "            k = torch.arange(1, d + 1).unsqueeze(1).repeat(1, T_c).t()\n",
    "            if self.use_cuda:\n",
    "                j, k = j.cuda(), k.cuda()\n",
    "\n",
    "            embeded_x_pe = []\n",
    "            for embed, J in zip(embeded_x, word_length.float()):  # iteration of n size\n",
    "                # embed: T_c d\n",
    "                # J: scalar\n",
    "                if J.eq(0).data[0]:  # all words are pad data, which means word_length = 0\n",
    "                    embeded_x_pe.append(embed)\n",
    "                else:\n",
    "                    l = (torch.ones_like(embed).float() - j / J) - (k / d) * (torch.ones_like(embed) - (2 * j) / J)\n",
    "                    embed = embed * l\n",
    "                    embeded_x_pe.append(embed)  # T_c, d\n",
    "            embeded_x_pe = torch.stack(embeded_x_pe)  # n, T_c, d\n",
    "            return embeded_x_pe.sum(1)  # n, d\n",
    "\n",
    "        else:\n",
    "            assert True, 'insert encoding_method key value in the model, default is \"basic\".'\n",
    "\n",
    "    def forward(self, stories, questions, stories_masks=None, questions_masks=None):\n",
    "        \"\"\"\n",
    "        stories, stories_masks: B, n, T_c\n",
    "        questions, questions_masks: B, T_q\n",
    "        \"\"\"\n",
    "        # init some requirements\n",
    "        te_idx_matrix = self._temporal_encoding_requirements(stories_masks)  # B, n\n",
    "        pe_word_lengths = self._pe_requirements(stories_masks)  # B, n\n",
    "\n",
    "        # Start Learning\n",
    "        o_list = []\n",
    "        # questions: B, T_q\n",
    "        embeded_B = self.embed_B(questions)  # B, T_q, d\n",
    "        u = embeded_B.sum(1)  # u: B, d\n",
    "        o_list.append(u)  # [(B, d)]\n",
    "\n",
    "        for k in range(self.n_hops):\n",
    "            # encoding part: PE, TE\n",
    "            batch_memories = []  # B, n, d\n",
    "            batch_contexts = []  # B, n, d\n",
    "            for i, inputs in enumerate(stories):  # iteration of batch\n",
    "                # inputs: n, T_c\n",
    "                embeded_A = self.embed_A(inputs)  # n, T_c, d\n",
    "                embeded_C = self.embed_C(inputs)  # n, T_c, d\n",
    "                # basic or PE\n",
    "                m = self.encoding2memory(embeded_A, pe_word_lengths[i])  # n, d\n",
    "                c = self.encoding2memory(embeded_C, pe_word_lengths[i])  # n, d\n",
    "                # TE\n",
    "                if self.te:\n",
    "                    A_T = self.embed_A_T(te_idx_matrix[i])  # n, d\n",
    "                    C_T = self.embed_C_T(te_idx_matrix[i])  # n, d\n",
    "                    m = m + A_T\n",
    "                    c = c + C_T\n",
    "                batch_memories.append(m)\n",
    "                batch_contexts.append(c)\n",
    "\n",
    "            batch_memories = torch.stack(batch_memories)  # B, n, d\n",
    "            batch_contexts = torch.stack(batch_contexts)  # B, n, d\n",
    "\n",
    "            # attention part: select which sentence to attent\n",
    "            # score = m * u[-1] : (B, n, d) * (B, d, 1) = B, n, 1\n",
    "            score = torch.bmm(batch_memories, o_list[-1].unsqueeze(2))\n",
    "            probs = F.softmax(score, dim=1)  # p: B, n, 1\n",
    "\n",
    "            # output: element-wies mul & sum (B, n, d) x (B, n, 1) = B, n, d --> B, d\n",
    "            o = torch.sum(batch_contexts * probs, 1)\n",
    "\n",
    "            o_next = o_list[-1] + o\n",
    "            o_list.append(o_next)  # B, d\n",
    "\n",
    "        # guessing part:\n",
    "        outputs = self.linear(o_list[-1])  # B, d > B, V\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Train_loader & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "bAbI_train = bAbIDataset(path_train, train=True, return_masks=True)\n",
    "train_loader = bAbIDataLoader(dataset=bAbI_train, batch_size=32, shuffle=True, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(bAbI_train.word2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.01\n",
    "STEP = 100\n",
    "MAX_STORY_LEN = bAbI_train.max_story_len\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING = False\n",
    "# ENCODING_METHOD = 'basic'\n",
    "# TEMPORAL = False\n",
    "ENCODING_METHOD = 'pe'\n",
    "TEMPORAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cuda(*args):\n",
    "    return [x.cuda() for x in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.5, milestones=[25, 50, 75], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 6]) torch.Size([32, 10, 6]) torch.Size([32, 4]) torch.Size([32, 1])\n",
      "tensor([[  6,   5,  12,   9,  14,   8],\n",
      "        [  6,   5,  12,   9,  14,  20],\n",
      "        [ 21,   5,  12,   9,  14,   7],\n",
      "        [ 21,   4,   9,  14,   8,   0],\n",
      "        [ 17,   5,   9,  14,   8,   0],\n",
      "        [ 18,   4,   9,  14,   8,   0],\n",
      "        [ 21,  15,   9,  14,  16,   0],\n",
      "        [ 21,  11,   9,  14,  13,   0],\n",
      "        [ 17,   5,  12,   9,  14,   3],\n",
      "        [ 18,  11,   9,  14,  20,   0]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    break\n",
    "\n",
    "print(stories.size(), stories_masks.size(), questions.size(), answers.size())\n",
    "print(stories[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] loss: 0.5016, lr: 0.01,\n",
      "[6/100] loss: 0.0001, lr: 0.01,\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    if EARLY_STOPPING:\n",
    "        break\n",
    "    for i, batch in enumerate(train_loader.load()):\n",
    "        stories, stories_masks, questions, _, answers, _ = batch\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            stories, stories_masks, questions, answers = get_cuda(stories, stories_masks, questions, answers)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(stories, questions, stories_masks=stories_masks)\n",
    "        \n",
    "        loss = loss_function(preds, answers.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {},'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        if np.mean(losses) < 0.01:\n",
    "            EARLY_STOPPING = True\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_path = '../model/E2EMN_basic.model'\n",
    "# ENCODING_METHOD = 'basic'\n",
    "# TEMPORAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '../model/E2EMN_te_pe.model'\n",
    "ENCODING_METHOD = 'pe'\n",
    "TEMPORAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6.4269e-02 -2.5233e-02  1.1140e-01  ...  -2.3988e-02 -2.4562e-01  5.6750e-02\n",
       "-8.3492e-02 -8.9224e-02 -5.1461e-02  ...  -2.8013e-02  4.8514e-02 -7.1548e-02\n",
       " 1.1526e-01  1.4142e-01 -9.9307e-02  ...  -9.3927e-02  9.8597e-02 -2.5389e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 2.6404e-02 -1.2081e-01 -2.3645e-01  ...   1.4338e-01  1.5789e-01  1.3981e-01\n",
       "-1.5928e-01  3.8540e-02 -9.7286e-02  ...   4.3691e-02  1.0589e-01 -1.3881e-01\n",
       " 9.6022e-02 -2.6435e-02 -1.9505e-01  ...   4.1171e-02  3.2750e-01 -7.2321e-02\n",
       "[torch.FloatTensor of size 22x50]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embed_A.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Soo/anaconda/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type E2EMN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, '../model/E2EMN_te_pe2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)\n",
    "test_loader = bAbIDataLoader(dataset=bAbI_test, batch_size=32, shuffle=False, to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "막 training 끝마치고 나서 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "    \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이상한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from model.bAbI_utils_loader import bAbIDataset, bAbIDataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, encoding_method='basic', temporal=True, \\\n",
    "                 use_cuda=False, max_story_len=None):\n",
    "        super(E2EMN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        # sharing matrix for k hops & and init to normal dist.\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        # TE: temporal encoding\n",
    "        if self.te:\n",
    "            assert max_story_len is not None, 'must have a fixed story_len, insert \"max_story_len\" as a number'\n",
    "            assert isinstance(max_story_len, int), '\"max_story_len\" must be a integer'\n",
    "            \n",
    "            self.embed_A_T = nn.Embedding(max_story_len+1, self.embed_size, padding_idx=0)\n",
    "            self.embed_C_T = nn.Embedding(max_story_len+1, self.embed_size, padding_idx=0)\n",
    "            if self.use_cuda:\n",
    "                self.embed_A_T = self.embed_A_T.cuda()\n",
    "                self.embed_C_T = self.embed_C_T.cuda()            \n",
    "            \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self._weight_init()\n",
    "    \n",
    "    def _weight_init(self):\n",
    "        for x in [self.embed_A, self.embed_B, self.embed_C]:\n",
    "            nn.init.normal(x.weight, mean=0, std=0.1)\n",
    "        if self.te:\n",
    "            for x in [self.embed_A_T, self.embed_C_T]:\n",
    "                nn.init.normal(x.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def _temporal_encoding_requirements(self, stories_masks):\n",
    "        # temporal encoding\n",
    "        if self.te:\n",
    "            story_len = stories_masks.size(1)\n",
    "            temp = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "            te_idx_matrix = torch.arange(1, story_len+1).repeat(temp.size(0)).view(temp.size()).long()\n",
    "            if self.use_cuda:\n",
    "                te_idx_matrix = te_idx_matrix.cuda()\n",
    "            te_idx_matrix = te_idx_matrix * temp.ge(1).long() # B, n\n",
    "        else:\n",
    "            te_idx_matrix = None\n",
    "            \n",
    "        return te_idx_matrix\n",
    "\n",
    "            \n",
    "    def _pe_requirements(self, stories_masks):\n",
    "        if stories_masks is not None:\n",
    "            pe_word_lengths = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "        else:\n",
    "            pe_word_lengths = None\n",
    "        return pe_word_lengths\n",
    "    \n",
    "    def encoding2memory(self, embeded_x, word_length=None):\n",
    "        \"\"\"\n",
    "        embed_x: n, T_c, d\n",
    "        word_length: n\n",
    "        \"\"\"\n",
    "        if self.encoding_method == 'basic':\n",
    "            return embeded_x.sum(1) # n, d\n",
    "        \n",
    "        elif self.encoding_method == 'pe':\n",
    "            assert word_length is not None, 'insert stories_masks when forward'\n",
    "            \n",
    "            T_c, d = embeded_x.size()[1:]\n",
    "            j = torch.arange(1, T_c+1).unsqueeze(1).repeat(1, d)\n",
    "            k = torch.arange(1, d+1).unsqueeze(1).repeat(1, T_c).t()\n",
    "            if self.use_cuda:\n",
    "                j, k = j.cuda(), k.cuda()\n",
    "                    \n",
    "            embeded_x_pe = []\n",
    "            for embed, J in zip(embeded_x, word_length.float()): # iteration of n size\n",
    "                # embed: T_c d\n",
    "                # J: scalar\n",
    "                if J.eq(0).data[0]: # all words are pad data, which means word_length = 0\n",
    "                    embeded_x_pe.append(embed)\n",
    "                else:\n",
    "                    l = (torch.ones_like(embed).float() - j/J) - (k/d)*(torch.ones_like(embed) - (2*j)/J)\n",
    "                    embed = embed * l\n",
    "                    embeded_x_pe.append(embed) # T_c, d\n",
    "            embeded_x_pe = torch.stack(embeded_x_pe) # n, T_c, d\n",
    "            return embeded_x_pe.sum(1) # n, d\n",
    "        \n",
    "        else:\n",
    "            assert True, 'insert encoding_method key value in the model, default is \"basic\".'\n",
    "        \n",
    "    def forward(self, stories, questions, stories_masks=None, questions_masks=None):\n",
    "        \"\"\"\n",
    "        stories, stories_masks: B, n, T_c\n",
    "        questions, questions_masks: B, T_q\n",
    "        \"\"\"\n",
    "        # init some requirements\n",
    "        te_idx_matrix = self._temporal_encoding_requirements(stories_masks)\n",
    "        pe_word_lengths = self._pe_requirements(stories_masks) # B, n \n",
    "        \n",
    "        # Start Learning\n",
    "        o_list = []\n",
    "        # questions: B, T_q\n",
    "        embeded_B = self.embed_B(questions) # B, T_q, d\n",
    "        u = embeded_B.sum(1) # u: B, d\n",
    "        o_list.append(u) # [(B, d)]\n",
    "        \n",
    "        for k in range(self.n_hops):\n",
    "            # encoding part: PE, TE\n",
    "            batch_memories = [] # B, n, d\n",
    "            batch_contexts = [] # B, n, d\n",
    "            for i, inputs in enumerate(stories): # iteration of batch\n",
    "                # inputs: n, T_c\n",
    "                embeded_A = self.embed_A(inputs) # n, T_c, d\n",
    "                embeded_C = self.embed_C(inputs)\n",
    "                # basic or PE\n",
    "                m = self.encoding2memory(embeded_A, pe_word_lengths[i]) # n, d\n",
    "                c = self.encoding2memory(embeded_C, pe_word_lengths[i]) # n, d\n",
    "                # TE\n",
    "                if self.te:\n",
    "                    A_T = self.embed_A_T(te_idx_matrix[i]) # n, d\n",
    "                    C_T = self.embed_C_T(te_idx_matrix[i]) # n, d\n",
    "                    m = m + A_T\n",
    "                    c = c + C_T\n",
    "                batch_memories.append(m)\n",
    "                batch_contexts.append(c)\n",
    "\n",
    "            batch_memories = torch.stack(batch_memories) # B, n, d\n",
    "            batch_contexts = torch.stack(batch_contexts) # B, n, d\n",
    "\n",
    "            # attention part: select which sentence to attent\n",
    "            # score = m * u[-1] : (B, n, d) * (B, d, 1) = B, n, 1\n",
    "            score = torch.bmm(batch_memories, o_list[-1].unsqueeze(2))\n",
    "            probs = F.softmax(score, dim=1) # p: B, n, 1\n",
    "\n",
    "            # output: element-wies mul & sum (B, n, d) x (B, n, 1) = B, n, d > B, d\n",
    "            o = torch.sum(batch_contexts * probs, 1)\n",
    "\n",
    "            o_next = o_list[-1] + o\n",
    "            o_list.append(o_next) # B, d\n",
    "        \n",
    "        # guessing part:\n",
    "        outputs = self.linear(o_list[-1]) # B, d > B, V\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "bAbI_train = bAbIDataset(path_train, train=True, return_masks=True)\n",
    "train_loader = bAbIDataLoader(dataset=bAbI_train, batch_size=32, shuffle=True, to_tensor=True)\n",
    "VOCAB_SIZE = len(bAbI_train.word2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.01\n",
    "STEP = 100\n",
    "MAX_STORY_LEN = bAbI_train.max_story_len\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING = False\n",
    "ENCODING_METHOD = 'pe'\n",
    "TEMPORAL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of E2EMN(\n",
       "  (embed_A): Embedding(22, 50, padding_idx=0)\n",
       "  (embed_B): Embedding(22, 50, padding_idx=0)\n",
       "  (embed_C): Embedding(22, 50, padding_idx=0)\n",
       "  (embed_A_T): Embedding(11, 50, padding_idx=0)\n",
       "  (embed_C_T): Embedding(11, 50, padding_idx=0)\n",
       "  (linear): Linear(in_features=50, out_features=22, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_A.weight', 'embed_B.weight', 'embed_C.weight', 'embed_A_T.weight', 'embed_C_T.weight', 'linear.weight', 'linear.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(model_path).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = '../model/E2EMN_te_pe.model'\n",
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6.4269e-02 -2.5233e-02  1.1140e-01  ...  -2.3988e-02 -2.4562e-01  5.6750e-02\n",
       "-8.3492e-02 -8.9224e-02 -5.1461e-02  ...  -2.8013e-02  4.8514e-02 -7.1548e-02\n",
       " 1.1526e-01  1.4142e-01 -9.9307e-02  ...  -9.3927e-02  9.8597e-02 -2.5389e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 2.6404e-02 -1.2081e-01 -2.3645e-01  ...   1.4338e-01  1.5789e-01  1.3981e-01\n",
       "-1.5928e-01  3.8540e-02 -9.7286e-02  ...   4.3691e-02  1.0589e-01 -1.3881e-01\n",
       " 9.6022e-02 -2.6435e-02 -1.9505e-01  ...   4.1171e-02  3.2750e-01 -7.2321e-02\n",
       "[torch.FloatTensor of size 22x50]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['embed_A.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)\n",
    "test_loader = bAbIDataLoader(dataset=bAbI_test, batch_size=32, shuffle=False, to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나갔다가 load state 하고 나온 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.187\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "    \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑어꺼는 무관"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic: 0.653\n",
    "* pe_te: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.choice(bAbI_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "story, q, a, s = bAbI_test.pad_to_story([random.choice(bAbI_test.data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: random print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "story, q, a, s = bAbI_test.pad_to_story([random.choice(bAbI_test.data)], w2idx)\n",
    "model.zero_grad()\n",
    "pred = model(story[0].unsqueeze(0), q)\n",
    "pred_a = torch.max(pred, 1)[1]\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('-'*45)\n",
    "print('\\n'.join([' '.join(list(map(lambda x: idx2w[x], f))) for f in story[0].data.tolist()]))\n",
    "print('-'*45)\n",
    "print(\"Question : \",' '.join(list(map(lambda x: idx2w[x], q.data.tolist()[0]))))\n",
    "print('-'*45)\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: idx2w[x], a.squeeze(1).data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: idx2w[x], pred_a.data.tolist()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
