{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.bAbI_utils import bAbI_data_loader, data_loader, pad_to_batch, pad_to_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "data, word2idx = bAbI_data_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for batch in data_loader(data, batch_size, shuffle=False):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories, stories_masks, questions, questions_masks, answers, supports = pad_to_batch(batch, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "data_test, word2idx = bAbI_data_loader(path_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for batch in data_loader(data_test, batch_size, shuffle=False):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[9, 3, 5, 15, 17],\n",
       "  [7, 2, 5, 15, 11],\n",
       "  [18, 4, 14, 5, 15, 11],\n",
       "  [9, 10, 5, 15, 13],\n",
       "  [9, 4, 5, 15, 17],\n",
       "  [19, 2, 5, 15, 20]],\n",
       " [12, 16, 19, 21],\n",
       " [20],\n",
       " 8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #4 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c141f053b27c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstories_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/gitproject/E2EMN/model/bAbI_utils.py\u001b[0m in \u001b[0;36mpad_to_batch\u001b[0;34m(batch, w2idx, no_batch)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstories_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mstory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mmax_story\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max_stories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# max_sentence_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #4 must support iteration"
     ]
    }
   ],
   "source": [
    "stories, stories_masks, questions, questions_masks, answers, supports = pad_to_batch(data_test[2], word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "   9   3   5  15  17   0\n",
       "   7   2   5  15  11   0\n",
       "   0   0   0   0   0   0\n",
       "   0   0   0   0   0   0\n",
       "\n",
       "(1 ,.,.) = \n",
       "   9   3   5  15  17   0\n",
       "   7   2   5  15  11   0\n",
       "  18   4  14   5  15  11\n",
       "   9  10   5  15  13   0\n",
       "[torch.LongTensor of size 2x4x6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: pad_to_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories, stories_masks, questions, questions_masks, answers, supports = pad_to_story(data_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories), len(stories_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions), len(questions_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers), len(supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class bAbIDataset(object):\n",
    "    def __init__(self, path, batch_size, train=True, vocab=None, sos=None, eos=None, return_masks=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.return_masks = return_masks\n",
    "        if self.train:\n",
    "            data, vocab = self.bAbI_data_loader(path, vocab=None, sos=sos, eos=eos)\n",
    "        else:\n",
    "            data, vocab = self.bAbI_data_loader(path, vocab=vocab, sos=sos, eos=eos)\n",
    "        \n",
    "        self.word2idx = vocab\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.data = data\n",
    "                \n",
    "    def data_loader(self, shuffle=False):\n",
    "        if shuffle: random.shuffle(self.data)\n",
    "        sindex = 0\n",
    "        eindex = self.batch_size\n",
    "        while eindex < len(self.data):\n",
    "            batch = self.data[sindex: eindex]\n",
    "            temp = eindex\n",
    "            eindex = eindex + self.batch_size\n",
    "            sindex = temp\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= len(self.data):\n",
    "            batch = self.data[sindex:]\n",
    "            yield batch\n",
    "        \n",
    "    def bAbI_data_loader(self, path, vocab=None, sos=None, eos=None):\n",
    "\n",
    "        assert ((isinstance(sos, str) and isinstance(eos, str)) == True) or ((sos and eos) is None), print('error')\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                data = file.readlines()\n",
    "            data = [l.strip() for l in data]\n",
    "        except:\n",
    "            print('no such file: {}'.format(path))\n",
    "            return None\n",
    "\n",
    "        data_temp = []\n",
    "        story = []\n",
    "\n",
    "        try:\n",
    "            for line in data:\n",
    "                idx, line = line.split(' ', 1)\n",
    "                if idx == '1':\n",
    "                    story = []\n",
    "\n",
    "                if '?' in line:\n",
    "                    q, a, support = line.split('\\t')\n",
    "                    q = q.lower().strip().replace('?', '').split() + ['?']\n",
    "                    a = a.lower().strip().split() + [eos] if eos else a.lower().strip().split()\n",
    "                    support = int(support)\n",
    "                    story_temp = deepcopy(story)\n",
    "                    data_temp.append([story_temp, q, a, support])\n",
    "                else:\n",
    "                    sentence = line.lower().replace('.', '').split() + [eos] if eos else line.lower().replace('.', '').split()\n",
    "                    story.append(sentence)\n",
    "        except:\n",
    "            print('check data')\n",
    "            return None\n",
    "\n",
    "        if vocab:\n",
    "            data, vocab = self.bAbI_build_vocab(data_temp, vocab)\n",
    "        else:\n",
    "            data, vocab = self.bAbI_build_vocab(data_temp, sos=sos, eos=eos)\n",
    "\n",
    "        return data, vocab\n",
    "\n",
    "    def bAbI_build_vocab(self, data, vocab=None, **kwargs):\n",
    "        if vocab is None:\n",
    "            sos = kwargs['sos']\n",
    "            eos = kwargs['eos']\n",
    "            story, q, a, s = list(zip(*data))\n",
    "            vocab = list(set(flatten(flatten(story)) + flatten(q) + flatten(a)))\n",
    "            if sos and eos:\n",
    "                word2idx = {'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3}\n",
    "            else:\n",
    "                word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "\n",
    "            for word in vocab:\n",
    "                if word2idx.get(word) is None:\n",
    "                    word2idx[word] = len(word2idx)\n",
    "        else:\n",
    "            word2idx = vocab\n",
    "\n",
    "        for d in data:\n",
    "            # d[0]: stories\n",
    "            # d[1]: questions\n",
    "            # d[2]: answer\n",
    "            # d[3]: support\n",
    "            for i, story in enumerate(d[0]):\n",
    "                d[0][i] = self._transfer2idx(story, word2idx)\n",
    "\n",
    "            d[1] = self._transfer2idx(d[1], word2idx)\n",
    "            d[2] = self._transfer2idx(d[2], word2idx)\n",
    "\n",
    "        return data, word2idx\n",
    "    \n",
    "    def _transfer2idx(self, seq, dictionary):\n",
    "        idxs = list(map(lambda w: dictionary[w] if dictionary.get(w) is not None else dictionary[\"<unk>\"], seq))\n",
    "        return idxs\n",
    "\n",
    "    def pad_to_batch(self, batch, w2idx, no_batch=False):\n",
    "        \"\"\"\n",
    "        stories, stories_masks: B, n, T_c\n",
    "        questions, questions_masks: B, T_q\n",
    "        answers: B, T_a\n",
    "        supports: B\n",
    "        -------------------------------------\n",
    "        output: stories, stories_masks, questions, questions_masks, answers, supports\n",
    "        \"\"\"\n",
    "        story, q, a, s = list(zip(*batch))\n",
    "        max_story = max([len(s) for s in story])  # max_stories\n",
    "        max_len = max([len(s) for s in flatten(story)])  # max_sentence_len\n",
    "        max_q = max([len(q_) for q_ in q])\n",
    "        max_a = max([len(a_) for a_ in a])\n",
    "\n",
    "        stories, stories_masks = [], []\n",
    "        for i in range(len(batch)):\n",
    "            story_array, story_mask = self.get_batch_array(self.get_fixed_array(story[i], w2idx), no_batch, max_story, max_len)\n",
    "            stories.append(story_array)\n",
    "            stories_masks.append(story_mask)\n",
    "\n",
    "        questions, questions_masks = self.get_batch_array(self.get_fixed_array(q, w2idx), no_batch, len(batch), max_q)\n",
    "        answers, _ = self.get_batch_array(self.get_fixed_array(a, w2idx), no_batch, len(batch), max_a)\n",
    "        \n",
    "        if self.return_masks:\n",
    "            return stories, stories_masks, questions, questions_masks, answers, s\n",
    "        else:\n",
    "            return stories, questions, answers, s\n",
    "        \n",
    "    def get_fixed_array(self, data, w2idx):\n",
    "        max_col = max([len(d) for d in data])\n",
    "        for j in range(len(data)):\n",
    "            if len(data[j]) < max_col:\n",
    "                data[j].append(w2idx.get('<pad>'))\n",
    "        return data\n",
    "\n",
    "    def get_batch_array(self, data, no_batch, *shape):\n",
    "        assert type(no_batch) == bool, 'no_batch, must be boolean'\n",
    "        r, c = shape\n",
    "        if no_batch:\n",
    "            r = len(data)\n",
    "        temp = np.zeros((r, c), dtype=np.int)\n",
    "        it = np.nditer(np.array(data, dtype=np.int), flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = np.array(data)[idx]\n",
    "            temp[idx] = tmp_val\n",
    "            it.iternext()\n",
    "\n",
    "        mask = (temp == 0).astype(np.byte)\n",
    "        return temp.tolist(), mask.tolist()\n",
    "    \n",
    "    def trans2tensor(self, x):\n",
    "        return Variable(torch.LongTensor(x))\n",
    "\n",
    "    def pad_to_story(test_data, w2idx, no_batch=True):  # this is for inference\n",
    "        \"\"\"\n",
    "        output: stories, stories_masks, questions, questions_masks, answers, supports\n",
    "        \"\"\"\n",
    "        story, q, a, s = list(zip(*test_data))\n",
    "        max_story = max([len(s) for s in story])  # max_stories\n",
    "        max_len = max([len(s) for s in flatten(story)])  # max_sentence_len\n",
    "        max_q = max([len(q_) for q_ in q])\n",
    "        max_a = max([len(a_) for a_ in a])\n",
    "\n",
    "        stories, stories_masks = [], []\n",
    "        for i in range(len(test_data)):\n",
    "            story_array, story_mask = self.get_batch_array(self.get_fixed_array(story[i], w2idx), no_batch, max_story, max_len)\n",
    "            stories.append(story_array)\n",
    "            stories_masks.append(story_mask)\n",
    "\n",
    "        questions, questions_masks = self.get_batch_array(self.get_fixed_array(q, w2idx), no_batch, len(test_data), max_q)\n",
    "        answers, _ = self.get_batch_array(self.get_fixed_array(a, w2idx), no_batch, len(test_data), max_a)\n",
    "        \n",
    "        if self.return_masks:\n",
    "            return stories, stories_masks, questions, questions_masks, answers, s\n",
    "        else:\n",
    "            return stories, questions, answers, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bAbI = bAbIDataset(path, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = bAbI.pad_to_batch(bAbI.data[:5], bAbI.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=bAbI, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
