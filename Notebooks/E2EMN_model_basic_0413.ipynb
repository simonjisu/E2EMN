{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/E2EMN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer\n",
    "\n",
    "### Sentences: \n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]: n \\times T_c$$\n",
    "\n",
    "* $n$: number of sentences in context\n",
    "* $T_c$: max length of a sentence in context\n",
    "\n",
    "### Embeding Matrix: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &: d \\times V \\\\\n",
    "B &: d \\times V \\\\\n",
    "C &: d \\times V\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j Ax_{ij}: T_c \\times d \\\\ \n",
    "c_i &= \\sum_j Cx_{ij}: T_c \\times d\\\\\n",
    "u &= \\sum_j Bq_{j}: T_q \\times d\n",
    "\\end{aligned}$$\n",
    "\n",
    "total embedding of context: $M : n \\times T_c \\times d$\n",
    "* $m_i(c_i)$: summation embedded for each sentence in context as length of $T_c$, $1 \\times d$\n",
    "* $u$: summation embedded for query(question) as length of $T_q$, $1 \\times d$\n",
    "* $score_i = m_iu^T: (1 \\times d) \\cdot (d \\times 1) = 1 \\times 1$\n",
    "\n",
    "### attention:\n",
    "$$\\begin{aligned}\n",
    "p_i &= softmax(score_i): 1 \\times 1 \\\\\n",
    "o_i &= c_i \\otimes p_i : d \\times 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "### summation vectors to linear layer:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "inputs &= u + o : d \\times 1 \\\\\n",
    "a &= softmax(W \\cdot inputs) : (V \\times d) \\times (d \\times 1) = V \\times 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postion encoding(PE):\n",
    "\n",
    "for each story(sentence) memory $m_i, c_i$\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j l_j \\otimes Ax_{ij}: T_c \\times d \\\\ \n",
    "l_{jk} &= (1-\\frac{j}{J}) - (\\frac{k}{d})(1-\\frac{2j}{J})\n",
    "\\end{aligned}$$\n",
    "\n",
    "remember, $l$ is a matrix that size is $T_c \\times d$\n",
    "\n",
    "* $J$: number of word in sentences\n",
    "* $j$: index of words\n",
    "* $d$: dimension of embedding\n",
    "* $k$: index of embedding dimension\n",
    "\n",
    "### temporal encoding(TE):\n",
    "* EX)\n",
    "> Sam walks into the kitchen.\n",
    ">\n",
    "> Sam picks up an apple.\n",
    ">\n",
    "> Sam walks into the bedroom. \n",
    ">\n",
    "> Sam drops the apple. \n",
    ">\n",
    "> Q: Where is the apple? \n",
    ">\n",
    "> A. Bedroom\n",
    "\n",
    "\n",
    "Many of the QA tasks require some notion of temporal context, i.e. the model needs to understand that Sam is in the bedroom after he is in the kitchen. To enable our model to address them, we modify the memory vector.\n",
    "\n",
    "$$m_i = \\sum_j l_j \\otimes Ax_{ij} + T_A(i)$$\n",
    "\n",
    "* $T_A(i)$: temporal encoding, size of $n \\times d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1503.08895.pdf\n",
    "\n",
    "https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from model.bAbI_utils_loader import bAbIDataset, bAbIDataLoader\n",
    "from model.E2EMN_model import E2EMN\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Train_loader & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "bAbI_train = bAbIDataset(path_train, train=True, return_masks=True)\n",
    "train_loader = bAbIDataLoader(dataset=bAbI_train, batch_size=32, shuffle=True, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(bAbI_train.word2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.01\n",
    "STEP = 100\n",
    "MAX_STORY_LEN = bAbI_train.max_story_len\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING = False\n",
    "ENCODING_METHOD = 'basic'\n",
    "TEMPORAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cuda(*args):\n",
    "    return [x.cuda() for x in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.5, milestones=[25, 50, 75], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] loss: 0.9052, lr: 0.01,\n",
      "[6/100] loss: 0.6717, lr: 0.01,\n",
      "[11/100] loss: 0.6718, lr: 0.01,\n",
      "[16/100] loss: 0.6605, lr: 0.01,\n",
      "[21/100] loss: 0.6549, lr: 0.01,\n",
      "[26/100] loss: 0.6228, lr: 0.005,\n",
      "[31/100] loss: 0.6199, lr: 0.005,\n",
      "[36/100] loss: 0.6169, lr: 0.005,\n",
      "[41/100] loss: 0.6195, lr: 0.005,\n",
      "[46/100] loss: 0.6164, lr: 0.005,\n",
      "[51/100] loss: 0.5980, lr: 0.0025,\n",
      "[56/100] loss: 0.5926, lr: 0.0025,\n",
      "[61/100] loss: 0.5950, lr: 0.0025,\n",
      "[66/100] loss: 0.5932, lr: 0.0025,\n",
      "[71/100] loss: 0.5930, lr: 0.0025,\n",
      "[76/100] loss: 0.5840, lr: 0.00125,\n",
      "[81/100] loss: 0.5809, lr: 0.00125,\n",
      "[86/100] loss: 0.5800, lr: 0.00125,\n",
      "[91/100] loss: 0.5790, lr: 0.00125,\n",
      "[96/100] loss: 0.5792, lr: 0.00125,\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    if EARLY_STOPPING:\n",
    "        break\n",
    "    for i, batch in enumerate(train_loader.load()):\n",
    "        stories, stories_masks, questions, _, answers, _ = batch\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            stories, stories_masks, questions, answers = get_cuda(stories, stories_masks, questions, answers)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(stories, questions, stories_masks=stories_masks)\n",
    "        \n",
    "        loss = loss_function(preds, answers.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {},'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        if np.mean(losses) < 0.01:\n",
    "            EARLY_STOPPING = True\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '../model/E2EMN_basic.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)\n",
    "test_loader = bAbIDataLoader(dataset=bAbI_test, batch_size=32, shuffle=False, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "    \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: random print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts : \n",
      "---------------------------------------------\n",
      "john travelled to the kitchen <pad>\n",
      "sandra moved to the bathroom <pad>\n",
      "daniel moved to the kitchen <pad>\n",
      "sandra moved to the kitchen <pad>\n",
      "daniel went to the hallway <pad>\n",
      "john went to the office <pad>\n",
      "sandra went back to the garden\n",
      "sandra went to the bedroom <pad>\n",
      "---------------------------------------------\n",
      "Question :  where is sandra ?\n",
      "---------------------------------------------\n",
      "Answer :  bedroom\n",
      "Prediction :  bedroom\n"
     ]
    }
   ],
   "source": [
    "idx2w = bAbI_test.idx2word\n",
    "story, mask, q, _, a, _ = bAbI_test.pad_to_story([random.choice(bAbI_test.data)])\n",
    "story, mask, q, a = [test_loader._to_tensor(x) for x in [story, mask, q, a]]\n",
    "model.zero_grad()\n",
    "pred = model(story, q, stories_masks=mask)\n",
    "pred_a = torch.max(pred, 1)[1]\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('-'*45)\n",
    "print('\\n'.join([' '.join(list(map(lambda x: idx2w[x], f))) for f in story[0].data.tolist()]))\n",
    "print('-'*45)\n",
    "print(\"Question : \",' '.join(list(map(lambda x: idx2w[x], q.data.tolist()[0]))))\n",
    "print('-'*45)\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: idx2w[x], a.squeeze(1).data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: idx2w[x], pred_a.data.tolist()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
