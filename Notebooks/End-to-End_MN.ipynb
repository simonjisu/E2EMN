{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from model.bAbI_utils import bAbI_data_loader, data_loader, pad_to_batch, pad_to_story\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/E2EMN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer\n",
    "Sentences: \n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]: n \\times T_c$$\n",
    "\n",
    "* $n$: number of sentences in context\n",
    "* $T_c$: max length of a sentence in context\n",
    "\n",
    "Embeding Matrix: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &: d \\times V \\\\\n",
    "B &: d \\times V \\\\\n",
    "C &: d \\times V\n",
    "\\end{aligned}$$\n",
    "\n",
    "inputs: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_i &: T_c \\times d \\\\ \n",
    "u &: T_q \\times d\n",
    "\\end{aligned}$$\n",
    "\n",
    "total embedding of context: $M : n \\times T_c \\times d$\n",
    "* $m_i(c_i)$: summation embedded for each sentence in context as length of $T_c$, $n \\times d$\n",
    "* $u$: summation embedded for query(question) as length of $T_q$, $1 \\times d$\n",
    "* $score = m_iu^T: (n \\times d) \\cdot (d \\times 1) = n \\times 1$\n",
    "\n",
    "attention:\n",
    "$$\\begin{aligned}\n",
    "p_i &= softmax(score): n \\times 1 \\\\\n",
    "o &= \\sum_i c_i p_i : d \\times 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "summation vectors to linear layer:\n",
    "$$\\begin{aligned}\n",
    "inputs = u + o : d \\times 1 \\\\\n",
    "a = softmax(W \\cdot inputs) : (V \\times d) \\times (d \\times 1) = V \\times 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1503.08895.pdf\n",
    "\n",
    "https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3):\n",
    "        super(E2EMN, self).__init__()\n",
    "\n",
    "        self.n_hops = n_hops\n",
    "\n",
    "        # sharing matrix for k hops\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, stories, questions):\n",
    "        \"\"\"\n",
    "        stories: B, n, T_c\n",
    "        questions: B, T_q\n",
    "        \"\"\"\n",
    "        o_list = []\n",
    "        # questions: B, T_q\n",
    "        embed_B = self.embed_B(questions) # B, T_q, d\n",
    "        u = embed_B.sum(1) # u^0: B, d\n",
    "        o_list.append(u) # [(B, d)]\n",
    "\n",
    "        for k in range(self.n_hops):\n",
    "            batch_memories = [] # B, n, d\n",
    "            batch_contexts = [] # B, n, d\n",
    "            for inputs in stories: \n",
    "                # inputs: n, T_c\n",
    "                embed_A = self.embed_A(inputs) # n, T_c, d\n",
    "                embed_C = self.embed_C(inputs)\n",
    "                m = embed_A.sum(1) # n, d\n",
    "                c = embed_C.sum(1) \n",
    "                batch_memories.append(m)\n",
    "                batch_contexts.append(c)\n",
    "\n",
    "            batch_memories = torch.stack(batch_memories) # B, n, d\n",
    "            batch_contexts = torch.stack(batch_contexts) # B, n, d\n",
    "\n",
    "            # attention: select which sentence to attent\n",
    "            score = torch.bmm(batch_memories, o_list[-1].unsqueeze(2)) # m * u[-1] : (B, n, d) x (B, d, 1) = B, n, 1\n",
    "            probs = F.softmax(score, dim=1) # p: B, n, 1\n",
    "\n",
    "            # output: element-wies mul & sum (B, n, d) x (B, n, 1) = B, n, d > B, d\n",
    "            o = torch.sum(batch_contexts * probs, 1)\n",
    "\n",
    "            o_next = o_list[-1] + o\n",
    "            o_list.append(o_next) # B, d\n",
    "\n",
    "        outputs = self.linear(o_list[-1]) # B, d > B, V\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "train_data, w2idx = bAbI_data_loader(path)\n",
    "train_loader = data_loader(train_data, batch_size=32, shuffle=True)\n",
    "idx2w = {v: k for k, v in w2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(w2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.001\n",
    "STEP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adm(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.5, milestones=[25, 50, 75], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cuda(*args):\n",
    "    return [x.cuda() for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] loss: 2.3857, lr: 0.001\n",
      "[6/100] loss: 1.5484, lr: 0.001\n",
      "[11/100] loss: 1.2506, lr: 0.001\n",
      "[16/100] loss: 1.0635, lr: 0.001\n",
      "[21/100] loss: 0.9784, lr: 0.001\n",
      "[26/100] loss: 0.9061, lr: 0.0005\n",
      "[31/100] loss: 0.8622, lr: 0.0005\n",
      "[36/100] loss: 0.8411, lr: 0.0005\n",
      "[41/100] loss: 0.8273, lr: 0.0005\n",
      "[46/100] loss: 0.8082, lr: 0.0005\n",
      "[51/100] loss: 0.7880, lr: 0.00025\n",
      "[56/100] loss: 0.7795, lr: 0.00025\n",
      "[61/100] loss: 0.7772, lr: 0.00025\n",
      "[66/100] loss: 0.7697, lr: 0.00025\n",
      "[71/100] loss: 0.7690, lr: 0.00025\n",
      "[76/100] loss: 0.7614, lr: 0.000125\n",
      "[81/100] loss: 0.7610, lr: 0.000125\n",
      "[86/100] loss: 0.7579, lr: 0.000125\n",
      "[91/100] loss: 0.7565, lr: 0.000125\n",
      "[96/100] loss: 0.7580, lr: 0.000125\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        stories, _, questions, _, answers, supports = pad_to_batch(batch, w2idx)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            stories, questions, answers = get_cuda(stories, questions, answers)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(stories, questions)\n",
    "        \n",
    "        loss = loss_function(preds, answers.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {}'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        losses=[]\n",
    "    train_loader = data_loader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "test_data, w2idx = bAbI_data_loader(path_test, w2idx)\n",
    "test_loader = data_loader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.629\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader):\n",
    "    stories, _, questions, _, answers, supports = pad_to_story(batch, w2idx)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "        \n",
    "    for story, q, a in zip(stories, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts : \n",
      "---------------------------------------------\n",
      "sandra went back to the bathroom\n",
      "sandra journeyed to the office <pad>\n",
      "---------------------------------------------\n",
      "Question :  where is sandra ?\n",
      "---------------------------------------------\n",
      "Answer :  office\n",
      "Prediction :  bathroom\n"
     ]
    }
   ],
   "source": [
    "story, _, q, _, a, s = pad_to_story([random.choice(test_data)], w2idx)\n",
    "model.zero_grad()\n",
    "pred = model(story[0].unsqueeze(0), q)\n",
    "pred_a = torch.max(pred, 1)[1]\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('-'*45)\n",
    "print('\\n'.join([' '.join(list(map(lambda x: idx2w[x], f))) for f in story[0].data.tolist()]))\n",
    "print('-'*45)\n",
    "print(\"Question : \",' '.join(list(map(lambda x: idx2w[x], q.data.tolist()[0]))))\n",
    "print('-'*45)\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: idx2w[x], a.squeeze(1).data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: idx2w[x], pred_a.data.tolist()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
