{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from model.bAbI_utils_loader import bAbIDataset, bAbIDataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/E2EMN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer\n",
    "\n",
    "### Sentences: \n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]: n \\times T_c$$\n",
    "\n",
    "* $n$: number of sentences in context\n",
    "* $T_c$: max length of a sentence in context\n",
    "\n",
    "### Embeding Matrix: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &: d \\times V \\\\\n",
    "B &: d \\times V \\\\\n",
    "C &: d \\times V\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j Ax_{ij}: T_c \\times d \\\\ \n",
    "c_i &= \\sum_j Cx_{ij}: T_c \\times d\\\\\n",
    "u &= \\sum_j Bq_{j}: T_q \\times d\n",
    "\\end{aligned}$$\n",
    "\n",
    "total embedding of context: $M : n \\times T_c \\times d$\n",
    "* $m_i(c_i)$: summation embedded for each sentence in context as length of $T_c$, $n \\times d$\n",
    "* $u$: summation embedded for query(question) as length of $T_q$, $1 \\times d$\n",
    "* $score = m_iu^T: (n \\times d) \\cdot (d \\times 1) = n \\times 1$\n",
    "\n",
    "### attention:\n",
    "$$\\begin{aligned}\n",
    "p_i &= softmax(score): n \\times 1 \\\\\n",
    "o &= \\sum_i c_i p_i : d \\times 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "### summation vectors to linear layer:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "inputs &= u + o : d \\times 1 \\\\\n",
    "a &= softmax(W \\cdot inputs) : (V \\times d) \\times (d \\times 1) = V \\times 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1503.08895.pdf\n",
    "\n",
    "https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postion encoding(PE):\n",
    "\n",
    "for each story(sentence) memory $m_i, c_i$\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j l_j \\otimes Ax_{ij}: T_c \\times d \\\\ \n",
    "l_{jk} &= (1-\\frac{j}{J}) - (\\frac{k}{d})(1-\\frac{2j}{J})\n",
    "\\end{aligned}$$\n",
    "\n",
    "remember, $l_j$ is a matrix that size is $T_c \\times d$\n",
    "\n",
    "* $J$: number of word in sentences\n",
    "* $j$: index of words\n",
    "* $d$: dimension of embedding\n",
    "* $k$: index of embedding dimension\n",
    "\n",
    "### temporal encoding(TE):\n",
    "\n",
    "for each story(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, encoding_method='basic', temporal=True, \\\n",
    "                 use_cuda=False):\n",
    "        super(E2EMN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        \n",
    "        # sharing matrix for k hops & and init to normal dist.\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "    \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def _weigth_init(self):\n",
    "        pass\n",
    "    \n",
    "    def _temporal_encoding_requirements(self, story_len, stories_masks):\n",
    "        # temporal encoding\n",
    "        if self.te:\n",
    "            assert story_len is not None, 'must have a fixed story_len, insert \"story_len\" as a number'\n",
    "            assert isinstance(story_len, int), '\"story_len\" must be a integer'\n",
    "            \n",
    "            self.embed_A_T = nn.Embedding(story_len+1, self.embed_size, padding_idx=0)\n",
    "            self.embed_C_T = nn.Embedding(story_len+1, self.embed_size, padding_idx=0)\n",
    "            \n",
    "            temp = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "            te_idx_matrix = Variable(torch.arange(1, story_len+1).repeat(temp.size(0)).view(temp.size()), \\\n",
    "                                     requires_grad=False).long()\n",
    "            te_idx_matrix = te_idx_matrix * temp.ge(1).long() # B, n\n",
    "            if self.use_cuda:\n",
    "                te_idx_matrix = te_idx_matrix.cuda()\n",
    "            return te_idx_matrix\n",
    "        else:\n",
    "            self.embed_A_T = None\n",
    "            self.embed_C_T = None\n",
    "            return None\n",
    "            \n",
    "    def _pe_requirements(self, stories_masks):\n",
    "        if stories_masks is not None:\n",
    "            pe_word_lengths = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "        else:\n",
    "            pe_word_lengths = None\n",
    "        return pe_word_lengths\n",
    "    \n",
    "    def encoding2memory(self, embeded_x, word_length=None):\n",
    "        \"\"\"\n",
    "        embed_x: n, T_c, d\n",
    "        word_length: n\n",
    "        \"\"\"\n",
    "        if self.encoding_method == 'basic':\n",
    "            return embeded_x.sum(1) # n, d\n",
    "        \n",
    "        elif self.encoding_method == 'pe':\n",
    "            assert word_length is not None, 'insert stories_masks when forward'\n",
    "            \n",
    "            T_c, d = embeded_x.size()[1:]\n",
    "            j = Variable(torch.arange(1, T_c+1).unsqueeze(1).repeat(1, d), requires_grad=False)\n",
    "            k = Variable(torch.arange(1, d+1).unsqueeze(1).repeat(1, T_c).t(), requires_grad=False)\n",
    "            if self.use_cuda:\n",
    "                j, k = j.cuda(), k.cuda()\n",
    "                    \n",
    "            embeded_x_pe = []\n",
    "            for embed, J in zip(embeded_x, word_length.float()): # iteration of n size\n",
    "                # embed: T_c d\n",
    "                # J: scalar\n",
    "                if J.eq(0).data[0]: # all words are pad data, which means word_length = 0\n",
    "                    embeded_x_pe.append(embed)\n",
    "                else:\n",
    "                    l = (torch.ones_like(embed).float() - j/J) - (k/d)*(torch.ones_like(embed) - (2*j)/J)\n",
    "                    embed = embed * l\n",
    "                    embeded_x_pe.append(embed) # T_c, d\n",
    "            embeded_x_pe = torch.stack(embeded_x_pe) # n, T_c, d\n",
    "            return embeded_x_pe.sum(1) # n, d\n",
    "        \n",
    "        else:\n",
    "            assert True, 'insert encoding_method key value in the model, default is \"basic\".'\n",
    "        \n",
    "    def forward(self, stories, questions, stories_masks=None, questions_masks=None, story_len=None):\n",
    "        \"\"\"\n",
    "        stories, stories_masks: B, n, T_c\n",
    "        questions, questions_masks: B, T_q\n",
    "        story_len = n\n",
    "        \"\"\"\n",
    "        # init some requirements\n",
    "        te_idx_matrix = self._temporal_encoding_requirements(story_len, stories_masks)\n",
    "        pe_word_lengths = self._pe_requirements(stories_masks) # B, n \n",
    "        \n",
    "        # Start Learning\n",
    "        o_list = []\n",
    "        # questions: B, T_q\n",
    "        embeded_B = self.embed_B(questions) # B, T_q, d\n",
    "        u = embeded_B.sum(1) # u: B, d\n",
    "        o_list.append(u) # [(B, d)]\n",
    "        \n",
    "        for k in range(self.n_hops):\n",
    "            # encoding part: PE, TE\n",
    "            batch_memories = [] # B, n, d\n",
    "            batch_contexts = [] # B, n, d\n",
    "            for i, inputs in enumerate(stories): # iteration of batch\n",
    "                # inputs: n, T_c\n",
    "                embeded_A = self.embed_A(inputs) # n, T_c, d\n",
    "                embeded_C = self.embed_C(inputs)\n",
    "                # basic or PE\n",
    "                m = self.encoding2memory(embeded_A, pe_word_lengths[i]) # n, d\n",
    "                c = self.encoding2memory(embeded_C, pe_word_lengths[i]) # n, d\n",
    "                # TE\n",
    "                if self.te:\n",
    "                    A_T = self.embed_A_T(te_idx_matrix[i]) # n, d\n",
    "                    C_T = self.embed_C_T(te_idx_matrix[i]) # n, d\n",
    "                    m = m + A_T\n",
    "                    c = c + C_T\n",
    "                batch_memories.append(m)\n",
    "                batch_contexts.append(c)\n",
    "\n",
    "            batch_memories = torch.stack(batch_memories) # B, n, d\n",
    "            batch_contexts = torch.stack(batch_contexts) # B, n, d\n",
    "\n",
    "            # attention part: select which sentence to attent\n",
    "            # score = m * u[-1] : (B, n, d) * (B, d, 1) = B, n, 1\n",
    "            score = torch.bmm(batch_memories, o_list[-1].unsqueeze(2))\n",
    "            probs = F.softmax(score, dim=1) # p: B, n, 1\n",
    "\n",
    "            # output: element-wies mul & sum (B, n, d) x (B, n, 1) = B, n, d > B, d\n",
    "            o = torch.sum(batch_contexts * probs, 1)\n",
    "\n",
    "            o_next = o_list[-1] + o\n",
    "            o_list.append(o_next) # B, d\n",
    "        \n",
    "        # guessing part:\n",
    "        outputs = self.linear(o_list[-1]) # B, d > B, V\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Train_loader & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "bAbI_train = bAbIDataset(path_train, train=True, return_masks=True)\n",
    "train_loader = bAbIDataLoader(dataset=bAbI_train, batch_size=32, shuffle=True, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(bAbI_train.word2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.01\n",
    "STEP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method='pe', temporal=True)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.5, milestones=[25, 50, 75], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cuda(*args):\n",
    "    return [x.cuda() for x in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2bd1ef2d86b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mstories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstories_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/gitproject/E2EMN/model/bAbI_utils_loader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mbatchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatchs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/gitproject/E2EMN/model/bAbI_utils_loader.py\u001b[0m in \u001b[0;36mpad_to_batch\u001b[0;34m(self, batch, w2idx, no_batch)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             story_array, story_mask = self.get_batch_array(self.get_fixed_array(story[i], w2idx), no_batch, max_story,\n\u001b[0;32m--> 136\u001b[0;31m                                                            max_len)\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mstories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mstories_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/gitproject/E2EMN/model/bAbI_utils_loader.py\u001b[0m in \u001b[0;36mget_batch_array\u001b[0;34m(self, data, no_batch, *shape)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader.load()):\n",
    "        stories, stories_masks, questions, _, answers, _ = batch\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            stories, stories_masks, questions, answers = get_cuda(stories, stories_masks, questions, answers)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(stories, questions, stories_masks=stories_masks, story_len=stories.size(1))\n",
    "        \n",
    "        loss = loss_function(preds, answers.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (step+1) % 10 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {}'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        losses=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/E2EMN_PE_TE.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=False)\n",
    "test_loader = bAbIDataLoader(dataset=bAbI_test, batch_size=32, shuffle=True, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.599\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "        \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0), story_len=story.size()[0])\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: random print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ea74224e47bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbAbI_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_story\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbAbI_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2idx' is not defined"
     ]
    }
   ],
   "source": [
    "story, q, a, s = bAbI_test.pad_to_story([random.choice(bAbI_test.data)], w2idx)\n",
    "model.zero_grad()\n",
    "pred = model(story[0].unsqueeze(0), q)\n",
    "pred_a = torch.max(pred, 1)[1]\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('-'*45)\n",
    "print('\\n'.join([' '.join(list(map(lambda x: idx2w[x], f))) for f in story[0].data.tolist()]))\n",
    "print('-'*45)\n",
    "print(\"Question : \",' '.join(list(map(lambda x: idx2w[x], q.data.tolist()[0]))))\n",
    "print('-'*45)\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: idx2w[x], a.squeeze(1).data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: idx2w[x], pred_a.data.tolist()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
