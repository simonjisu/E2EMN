{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from model.bAbI_utils_loader import bAbIDataset, bAbIDataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/E2EMN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer\n",
    "\n",
    "### Sentences: \n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]: n \\times T_c$$\n",
    "\n",
    "* $n$: number of sentences in context\n",
    "* $T_c$: max length of a sentence in context\n",
    "\n",
    "### Embeding Matrix: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &: d \\times V \\\\\n",
    "B &: d \\times V \\\\\n",
    "C &: d \\times V\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j Ax_{ij}: T_c \\times d \\\\ \n",
    "c_i &= \\sum_j Cx_{ij}: T_c \\times d\\\\\n",
    "u &= \\sum_j Bq_{j}: T_q \\times d\n",
    "\\end{aligned}$$\n",
    "\n",
    "total embedding of context: $M : n \\times T_c \\times d$\n",
    "* $m_i(c_i)$: summation embedded for each sentence in context as length of $T_c$, $n \\times d$\n",
    "* $u$: summation embedded for query(question) as length of $T_q$, $1 \\times d$\n",
    "* $score = m_iu^T: (n \\times d) \\cdot (d \\times 1) = n \\times 1$\n",
    "\n",
    "### attention:\n",
    "$$\\begin{aligned}\n",
    "p_i &= softmax(score): n \\times 1 \\\\\n",
    "o &= \\sum_i c_i p_i : d \\times 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "### summation vectors to linear layer:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "inputs &= u + o : d \\times 1 \\\\\n",
    "a &= softmax(W \\cdot inputs) : (V \\times d) \\times (d \\times 1) = V \\times 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1503.08895.pdf\n",
    "\n",
    "https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postion encoding(PE):\n",
    "\n",
    "for each story(sentence) memory $m_i, c_i$\n",
    "$$\\begin{aligned}\n",
    "m_i &= \\sum_j l_j \\otimes Ax_{ij}: T_c \\times d \\\\ \n",
    "l_{jk} &= (1-\\frac{j}{J}) - (\\frac{k}{d})(1-\\frac{2j}{J})\n",
    "\\end{aligned}$$\n",
    "\n",
    "remember, $l_j$ is a matrix that size is $T_c \\times d$\n",
    "\n",
    "* $J$: number of word in sentences\n",
    "* $j$: index of words\n",
    "* $d$: dimension of embedding\n",
    "* $k$: index of embedding dimension\n",
    "\n",
    "### temporal encoding(TE):\n",
    "\n",
    "for each story(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, encoding_method='basic', temporal=True, \\\n",
    "                 use_cuda=False, max_story_len=None):\n",
    "        super(E2EMN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        # sharing matrix for k hops & and init to normal dist.\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "        # TE: temporal encoding\n",
    "        if self.te:\n",
    "            assert max_story_len is not None, 'must have a fixed story_len, insert \"max_story_len\" as a number'\n",
    "            assert isinstance(max_story_len, int), '\"max_story_len\" must be a integer'\n",
    "            \n",
    "            self.embed_A_T = nn.Embedding(max_story_len+1, self.embed_size, padding_idx=0)\n",
    "            self.embed_C_T = nn.Embedding(max_story_len+1, self.embed_size, padding_idx=0)\n",
    "            if self.use_cuda:\n",
    "                self.embed_A_T = self.embed_A_T.cuda()\n",
    "                self.embed_C_T = self.embed_C_T.cuda()            \n",
    "            \n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self._weight_init()\n",
    "    \n",
    "    def _weight_init(self):\n",
    "        for x in [self.embed_A, self.embed_B, self.embed_C]:\n",
    "            nn.init.normal(x.weight, mean=0, std=0.1)\n",
    "        if self.te:\n",
    "            for x in [self.embed_A_T, self.embed_C_T]:\n",
    "                nn.init.normal(x.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def _temporal_encoding_requirements(self, stories_masks):\n",
    "        # temporal encoding\n",
    "        if self.te:\n",
    "            story_len = stories_masks.size(1)\n",
    "            temp = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "            te_idx_matrix = Variable(torch.arange(1, story_len+1).repeat(temp.size(0)).view(temp.size()), \\\n",
    "                                     requires_grad=False).long()\n",
    "            if self.use_cuda:\n",
    "                te_idx_matrix = te_idx_matrix.cuda()\n",
    "            te_idx_matrix = te_idx_matrix * temp.ge(1).long() # B, n\n",
    "        else:\n",
    "            te_idx_matrix = None\n",
    "            \n",
    "        return te_idx_matrix\n",
    "\n",
    "            \n",
    "    def _pe_requirements(self, stories_masks):\n",
    "        if stories_masks is not None:\n",
    "            pe_word_lengths = stories_masks.eq(0).sum(2) # B, n : byte tensor\n",
    "        else:\n",
    "            pe_word_lengths = None\n",
    "        return pe_word_lengths\n",
    "    \n",
    "    def encoding2memory(self, embeded_x, word_length=None):\n",
    "        \"\"\"\n",
    "        embed_x: n, T_c, d\n",
    "        word_length: n\n",
    "        \"\"\"\n",
    "        if self.encoding_method == 'basic':\n",
    "            return embeded_x.sum(1) # n, d\n",
    "        \n",
    "        elif self.encoding_method == 'pe':\n",
    "            assert word_length is not None, 'insert stories_masks when forward'\n",
    "            \n",
    "            T_c, d = embeded_x.size()[1:]\n",
    "            j = Variable(torch.arange(1, T_c+1).unsqueeze(1).repeat(1, d), requires_grad=False)\n",
    "            k = Variable(torch.arange(1, d+1).unsqueeze(1).repeat(1, T_c).t(), requires_grad=False)\n",
    "            if self.use_cuda:\n",
    "                j, k = j.cuda(), k.cuda()\n",
    "                    \n",
    "            embeded_x_pe = []\n",
    "            for embed, J in zip(embeded_x, word_length.float()): # iteration of n size\n",
    "                # embed: T_c d\n",
    "                # J: scalar\n",
    "                if J.eq(0).data[0]: # all words are pad data, which means word_length = 0\n",
    "                    embeded_x_pe.append(embed)\n",
    "                else:\n",
    "                    l = (torch.ones_like(embed).float() - j/J) - (k/d)*(torch.ones_like(embed) - (2*j)/J)\n",
    "                    embed = embed * l\n",
    "                    embeded_x_pe.append(embed) # T_c, d\n",
    "            embeded_x_pe = torch.stack(embeded_x_pe) # n, T_c, d\n",
    "            return embeded_x_pe.sum(1) # n, d\n",
    "        \n",
    "        else:\n",
    "            assert True, 'insert encoding_method key value in the model, default is \"basic\".'\n",
    "        \n",
    "    def forward(self, stories, questions, stories_masks=None, questions_masks=None):\n",
    "        \"\"\"\n",
    "        stories, stories_masks: B, n, T_c\n",
    "        questions, questions_masks: B, T_q\n",
    "        \"\"\"\n",
    "        # init some requirements\n",
    "        te_idx_matrix = self._temporal_encoding_requirements(stories_masks)\n",
    "        pe_word_lengths = self._pe_requirements(stories_masks) # B, n \n",
    "        \n",
    "        # Start Learning\n",
    "        o_list = []\n",
    "        # questions: B, T_q\n",
    "        embeded_B = self.embed_B(questions) # B, T_q, d\n",
    "        u = embeded_B.sum(1) # u: B, d\n",
    "        o_list.append(u) # [(B, d)]\n",
    "        \n",
    "        for k in range(self.n_hops):\n",
    "            # encoding part: PE, TE\n",
    "            batch_memories = [] # B, n, d\n",
    "            batch_contexts = [] # B, n, d\n",
    "            for i, inputs in enumerate(stories): # iteration of batch\n",
    "                # inputs: n, T_c\n",
    "                embeded_A = self.embed_A(inputs) # n, T_c, d\n",
    "                embeded_C = self.embed_C(inputs)\n",
    "                # basic or PE\n",
    "                m = self.encoding2memory(embeded_A, pe_word_lengths[i]) # n, d\n",
    "                c = self.encoding2memory(embeded_C, pe_word_lengths[i]) # n, d\n",
    "                # TE\n",
    "                if self.te:\n",
    "                    A_T = self.embed_A_T(te_idx_matrix[i]) # n, d\n",
    "                    C_T = self.embed_C_T(te_idx_matrix[i]) # n, d\n",
    "                    m = m + A_T\n",
    "                    c = c + C_T\n",
    "                batch_memories.append(m)\n",
    "                batch_contexts.append(c)\n",
    "\n",
    "            batch_memories = torch.stack(batch_memories) # B, n, d\n",
    "            batch_contexts = torch.stack(batch_contexts) # B, n, d\n",
    "\n",
    "            # attention part: select which sentence to attent\n",
    "            # score = m * u[-1] : (B, n, d) * (B, d, 1) = B, n, 1\n",
    "            score = torch.bmm(batch_memories, o_list[-1].unsqueeze(2))\n",
    "            probs = F.softmax(score, dim=1) # p: B, n, 1\n",
    "\n",
    "            # output: element-wies mul & sum (B, n, d) x (B, n, 1) = B, n, d > B, d\n",
    "            o = torch.sum(batch_contexts * probs, 1)\n",
    "\n",
    "            o_next = o_list[-1] + o\n",
    "            o_list.append(o_next) # B, d\n",
    "        \n",
    "        # guessing part:\n",
    "        outputs = self.linear(o_list[-1]) # B, d > B, V\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Train_loader & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt'\n",
    "bAbI_train = bAbIDataset(path_train, train=True, return_masks=True)\n",
    "train_loader = bAbIDataLoader(dataset=bAbI_train, batch_size=32, shuffle=True, to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(bAbI_train.word2idx)\n",
    "EMBED_SIZE = 50\n",
    "N_HOPS = 3\n",
    "LR = 0.01\n",
    "STEP = 100\n",
    "MAX_STORY_LEN = bAbI_train.max_story_len\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOPPING = False\n",
    "# ENCODING_METHOD = 'basic'\n",
    "# TEMPORAL = False\n",
    "ENCODING_METHOD = 'pe'\n",
    "TEMPORAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cuda(*args):\n",
    "    return [x.cuda() for x in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings: Loss Function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.5, milestones=[25, 50, 75], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] loss: 0.4770, lr: 0.01,\n",
      "[6/100] loss: 0.0001, lr: 0.01,\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    if EARLY_STOPPING:\n",
    "        break\n",
    "    for i, batch in enumerate(train_loader.load()):\n",
    "        stories, stories_masks, questions, _, answers, _ = batch\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            stories, stories_masks, questions, answers = get_cuda(stories, stories_masks, questions, answers)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(stories, questions, stories_masks=stories_masks)\n",
    "        \n",
    "        loss = loss_function(preds, answers.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {},'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        if np.mean(losses) < 0.01:\n",
    "            EARLY_STOPPING = True\n",
    "            print(\"Early Stopping!\")\n",
    "            break\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_path = '../model/E2EMN_basic.model'\n",
    "# ENCODING_METHOD = 'basic'\n",
    "# TEMPORAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '../model/E2EMN_te_pe.model'\n",
    "ENCODING_METHOD = 'pe'\n",
    "TEMPORAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed_A.weight', \n",
       "               8.7212e-02 -2.4311e-02 -5.3458e-02  ...  -1.0072e-01 -7.0019e-02 -8.0092e-02\n",
       "              -1.2276e-01  8.6810e-03  5.4344e-02  ...   1.3872e-01 -1.1800e-03  6.2671e-02\n",
       "              -1.4740e-01  1.6852e-01 -1.3936e-01  ...   1.6313e-01  9.0611e-02 -4.2169e-01\n",
       "                              ...                   ⋱                   ...                \n",
       "              -2.3257e-01 -1.2822e-01  5.6160e-01  ...   2.9820e-01  1.0062e-01  1.2733e-01\n",
       "               2.0542e-02 -6.0076e-02 -2.2000e-02  ...   2.1464e-02  4.2375e-02  7.6490e-02\n",
       "              -1.7213e-05  8.0307e-02 -1.4132e-02  ...  -3.1378e-02 -1.9798e-01 -4.4935e-02\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_B.weight', \n",
       "              -0.0114 -0.0028  0.0633  ...  -0.0429  0.1092  0.1136\n",
       "               0.0152 -0.1037 -0.2155  ...  -0.0356 -0.0099 -0.1166\n",
       "              -0.0591  0.0252  0.1548  ...  -0.0778  0.0614  0.0145\n",
       "                        ...             ⋱             ...          \n",
       "              -0.4203 -0.2356  0.2772  ...   0.2522  0.1150  0.0101\n",
       "              -0.1467  0.2086 -0.0034  ...   0.2003  0.1051 -0.0217\n",
       "              -0.1650  0.0674  0.0208  ...   0.0020 -0.0461 -0.0029\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_C.weight', \n",
       "              -0.1772 -0.0378  0.0515  ...   0.2341 -0.1668 -0.0509\n",
       "              -0.2220 -0.1890  0.0266  ...   0.1400 -0.1312  0.0242\n",
       "              -0.1204  0.1516 -0.1542  ...   0.0787 -0.2124 -0.0711\n",
       "                        ...             ⋱             ...          \n",
       "              -0.0731  0.0177  0.2357  ...   0.0482 -0.1711 -0.1522\n",
       "               0.1831  0.0105  0.0738  ...  -0.0102  0.0692 -0.1884\n",
       "               0.2193  0.9664 -0.5530  ...  -0.3732 -0.8419  0.9935\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_A_T.weight', \n",
       "              \n",
       "              Columns 0 to 9 \n",
       "              -0.0588 -0.0688 -0.0664 -0.0541  0.2300  0.0692 -0.0114 -0.1658  0.0815  0.0111\n",
       "               0.4951 -0.3592  0.1473  0.3761  0.0944 -0.1153 -0.0215  0.1104  0.0844  0.3843\n",
       "               0.1770 -0.2283 -0.0100  0.0789 -0.1066  0.1780  0.0416  0.0429  0.3036  0.1992\n",
       "              -0.0453 -0.2125  0.2530  0.0144  0.0800  0.1098 -0.1930  0.0598  0.3144  0.1533\n",
       "               0.0882 -0.0942  0.1722 -0.0319  0.0292  0.0786 -0.1632 -0.2797 -0.0644  0.2717\n",
       "               0.2738  0.0802  0.1392  0.2070  0.0719  0.0195 -0.0380 -0.1765 -0.2330  0.0024\n",
       "              -0.1017  0.2673 -0.3322 -0.0220 -0.2454 -0.1694 -0.0324  0.1177 -0.1571 -0.0816\n",
       "              -0.2166  0.3913 -0.4236 -0.0028 -0.1201 -0.0120  0.2311  0.1433 -0.1676 -0.2430\n",
       "              -0.3892  0.0989 -0.3070 -0.1489  0.0476 -0.1538  0.4739 -0.1177 -0.1213 -0.2030\n",
       "              -0.1321  0.2295 -0.1497 -0.3349 -0.0104 -0.2844  0.2849 -0.1953 -0.4430 -0.3144\n",
       "              -0.4900  0.4258 -0.3665 -0.5024  0.0985 -0.2033  0.3065  0.1422 -0.2476 -0.8226\n",
       "              \n",
       "              Columns 10 to 19 \n",
       "              -0.1997 -0.0293  0.1595 -0.0263  0.0486 -0.1024  0.0698 -0.0040 -0.1500  0.0681\n",
       "              -0.0920 -0.3544 -0.0224  0.1585  0.3361 -0.3899  0.3863 -0.3251 -0.1247  0.5929\n",
       "               0.1923 -0.2080 -0.0529  0.1220  0.1999 -0.2145  0.3563 -0.2784 -0.0435  0.3388\n",
       "               0.1791  0.0073 -0.0068  0.1019  0.0570 -0.2660  0.0119 -0.1284 -0.0861  0.2046\n",
       "               0.1317  0.0928  0.2691 -0.0852  0.0363  0.1299  0.0099 -0.1191 -0.0932  0.0061\n",
       "              -0.1547  0.0305  0.1129  0.1343 -0.0335 -0.1197 -0.2192  0.1813  0.0356 -0.1240\n",
       "               0.3325  0.1823 -0.0309 -0.0624  0.0623  0.1642 -0.0630 -0.1476  0.3401 -0.1219\n",
       "               0.2772  0.2446  0.1355 -0.0059 -0.1863  0.2879 -0.1398 -0.0657  0.2895 -0.1088\n",
       "               0.0540  0.2366  0.0656 -0.0385 -0.3467  0.2836 -0.1423 -0.1369 -0.1701 -0.2332\n",
       "              -0.1773  0.4941  0.0311 -0.1387 -0.3348  0.3187 -0.2383  0.5089  0.2002 -0.4756\n",
       "              -0.1232  0.3716 -0.3475 -0.0353 -0.1392  0.5692 -0.2267  0.1290  0.3928 -0.5044\n",
       "              \n",
       "              Columns 20 to 29 \n",
       "               0.0932 -0.0183  0.0185  0.1330 -0.3078  0.2236 -0.1047 -0.1347  0.0451 -0.0960\n",
       "              -0.5329  0.2923  0.0585  0.1817  0.0123  0.0124 -0.4210  0.1674  0.4039 -0.1659\n",
       "              -0.0365  0.1712 -0.0665  0.3987 -0.0566 -0.0296 -0.0851  0.3213  0.2117 -0.0091\n",
       "              -0.0042  0.2495 -0.0721  0.0011  0.0513 -0.1976  0.3409 -0.3340  0.1475  0.2234\n",
       "              -0.0616 -0.1080  0.1009 -0.3681 -0.0045  0.1140 -0.0800 -0.1760 -0.0166  0.1005\n",
       "               0.1559 -0.1814  0.1801 -0.1056  0.0604  0.0815 -0.0974 -0.2331 -0.0973 -0.1163\n",
       "               0.1498 -0.1464  0.1865  0.0209 -0.1248  0.0531  0.1329  0.1678  0.0108 -0.2048\n",
       "               0.3504 -0.1893  0.2918 -0.1057 -0.0276 -0.0643 -0.0664  0.0276 -0.0429  0.0969\n",
       "               0.4167 -0.2782  0.3058 -0.5401  0.1301 -0.0828  0.0788  0.2760 -0.1867 -0.2010\n",
       "               0.5549 -0.2057  0.0966 -0.2059  0.4710 -0.2069  0.4855 -0.1106 -0.1620 -0.0641\n",
       "               0.5793 -0.3850  0.1584 -0.4030  0.1358 -0.4313  0.4784  0.0080 -0.1740 -0.2456\n",
       "              \n",
       "              Columns 30 to 39 \n",
       "               0.0564  0.0419 -0.0365  0.0374 -0.0154 -0.0770  0.0004  0.0232  0.0716 -0.0161\n",
       "              -0.2836  0.2343 -0.3269  0.0779 -0.1950  0.2385 -0.2775  0.5109 -0.5399 -0.3524\n",
       "              -0.2595  0.1615 -0.2134 -0.1791 -0.2308 -0.0281 -0.4392  0.4897 -0.3567  0.0526\n",
       "              -0.0392  0.1243  0.2408  0.1987 -0.2276  0.0092 -0.1612  0.1368 -0.1398 -0.1863\n",
       "              -0.0329  0.1521  0.1777  0.0003 -0.0216  0.0210 -0.0478  0.0892 -0.0301 -0.3777\n",
       "              -0.0994 -0.1009 -0.0011 -0.1358  0.2820 -0.0312 -0.0913 -0.1236  0.1106 -0.1213\n",
       "              -0.2222 -0.1131 -0.2496 -0.2683  0.2616 -0.0359  0.2061 -0.2099  0.0420  0.1009\n",
       "              -0.1788 -0.1326  0.0424 -0.2426  0.2152  0.0473  0.2124 -0.2250  0.0468  0.1102\n",
       "              -0.2066 -0.1295  0.3110  0.0946  0.3181  0.0968  0.3913 -0.3912  0.2346  0.3136\n",
       "               0.3362 -0.2936  0.4817 -0.1671  0.3492 -0.0968  0.3533 -0.4362  0.3492  0.4397\n",
       "               0.3786 -0.3016  0.3834 -0.2202  0.4405 -0.2165  0.3168 -0.6214  0.6713  0.2891\n",
       "              \n",
       "              Columns 40 to 49 \n",
       "               0.0666  0.0234  0.0113  0.0455  0.1141 -0.0157  0.0189 -0.1354 -0.0167  0.0114\n",
       "               0.3667 -0.1619  0.3488  0.4821 -0.1563 -0.0307  0.4092 -0.4442  0.2327  0.5301\n",
       "              -0.1771 -0.2407  0.1591  0.4240  0.0876 -0.0657  0.1274 -0.3697 -0.0864  0.0728\n",
       "              -0.2988 -0.1188  0.2258  0.3697 -0.0572  0.1313  0.1659 -0.3249 -0.0725 -0.0826\n",
       "               0.0922 -0.0657  0.0522 -0.1166 -0.1925 -0.1269  0.1736 -0.1764  0.0787 -0.2014\n",
       "               0.3879 -0.2116 -0.0437 -0.2143 -0.0056  0.1265 -0.1745 -0.1272 -0.0790  0.0012\n",
       "               0.2218  0.1895 -0.0968 -0.0095  0.0748  0.1454 -0.2297  0.0393  0.1353 -0.1435\n",
       "              -0.0796  0.4546 -0.2188 -0.3094  0.3743 -0.0341 -0.3211  0.3198 -0.1762 -0.1655\n",
       "              -0.3294  0.2475 -0.3689 -0.3941  0.3252  0.3834 -0.3192  0.3548 -0.3149 -0.5185\n",
       "              -0.2034  0.1065 -0.4359 -0.5217  0.2701  0.3499 -0.3392  0.3339 -0.3226 -0.4167\n",
       "              -0.2967  0.5313 -0.6216 -0.4606  0.0782  0.1327 -0.5646  0.5114 -0.2761 -0.6384\n",
       "              [torch.FloatTensor of size 11x50]), ('embed_C_T.weight', \n",
       "              \n",
       "              Columns 0 to 9 \n",
       "               0.0288  0.0499 -0.0834 -0.0649  0.0644  0.1646  0.1544 -0.0558 -0.0002 -0.0200\n",
       "              -0.2045 -0.1601 -0.0073  0.1596 -0.0815  0.2384  0.1204 -0.0597  0.0609 -0.2080\n",
       "              -0.1891 -0.0128  0.2653 -0.1137  0.1117  0.0840 -0.1884  0.2661  0.4693  0.0487\n",
       "              -0.0526  0.1441  0.3619  0.0947  0.3413 -0.0683 -0.1006 -0.2511 -0.4082  0.1239\n",
       "               0.4442  0.3713 -0.0095  0.1978  0.0207 -0.1729 -0.0188 -0.0892 -0.4551  0.1195\n",
       "              -0.1715  0.4341 -0.3666  0.0605 -0.2256 -0.2228  0.2314  0.2962 -0.2640 -0.2922\n",
       "              -0.4057  0.1776 -0.4001  0.0127 -0.2375 -0.0128  0.1054  0.2212  0.2729  0.2042\n",
       "              -0.3559 -0.3081  0.0324 -0.0260  0.3801  0.0579  0.6589 -0.2140  0.0878  0.0697\n",
       "              -0.1260 -0.3421  0.3757 -0.3130  0.2886 -0.1978  0.1849 -0.1668  0.0080  0.0403\n",
       "              -0.2115  0.1146  0.0492 -0.3963  0.0280 -0.1753 -0.1225  0.1283 -0.1148 -0.4282\n",
       "              -0.3024  0.1277 -0.3434  0.0098  0.2385  0.1171  0.2174  0.3149  0.3507 -0.4088\n",
       "              \n",
       "              Columns 10 to 19 \n",
       "              -0.0327 -0.0245  0.0876 -0.0670  0.0493 -0.1325  0.0583  0.1315 -0.0307 -0.0070\n",
       "               0.2108 -0.0136 -0.2877  0.1939  0.0813  0.1409 -0.0039 -0.0496  0.1705  0.1027\n",
       "               0.0967  0.0443  0.2206 -0.0226 -0.1113 -0.1244 -0.0624 -0.0231 -0.1981 -0.0346\n",
       "              -0.0995  0.2867  0.3574 -0.0365 -0.2269  0.1091 -0.3567  0.1539 -0.0648 -0.1166\n",
       "              -0.0601  0.0940  0.0749  0.1369 -0.0095  0.0981 -0.2166  0.1241  0.0874 -0.1812\n",
       "               0.0180  0.1639 -0.3372  0.2948  0.0593 -0.0738 -0.1120  0.1792  0.3867  0.0947\n",
       "               0.5082  0.0723  0.0147 -0.1278 -0.1375  0.2164  0.2984 -0.3755  0.0959  0.1535\n",
       "              -0.2769 -0.0504  0.1296  0.0877 -0.5519 -0.1329  0.1568 -0.0891 -0.6430  0.1782\n",
       "              -0.2380  0.1716 -0.2209 -0.0336 -0.1324  0.1227 -0.1074  0.2493 -0.1747 -0.1558\n",
       "              -0.2274  0.3234 -0.4533  0.1599  0.1739  0.1474 -0.1683  0.3166  0.4361 -0.2296\n",
       "               0.1934 -0.2954 -0.4371  0.1506  0.0992  0.0607 -0.1460 -0.3094 -0.0816 -0.0729\n",
       "              \n",
       "              Columns 20 to 29 \n",
       "              -0.1323  0.1378 -0.0537  0.1608  0.0823  0.0005  0.0726  0.1413  0.0220  0.0475\n",
       "               0.0823 -0.0059  0.0175  0.0217  0.0984  0.1390  0.0015  0.1602  0.0304 -0.1291\n",
       "               0.0443  0.2735 -0.2336  0.0697 -0.0070 -0.1531  0.4155 -0.2834 -0.0294  0.1468\n",
       "              -0.0513 -0.2861  0.0939 -0.5902  0.0125  0.2282 -0.1232 -0.4668 -0.0458  0.1614\n",
       "              -0.1040 -0.1404  0.1713  0.0496  0.0715  0.1700 -0.6452 -0.1369 -0.0732 -0.3125\n",
       "               0.1679  0.0253  0.0663  0.3221 -0.0683  0.0281  0.0618  0.1092 -0.0606 -0.2967\n",
       "              -0.2213  0.0690  0.1949  0.1376 -0.2565  0.1861 -0.1890  0.1549  0.2419  0.0561\n",
       "               0.1912 -0.0295  0.1962 -0.3753  0.1277  0.1394 -0.3867 -0.0130 -0.0640 -0.0174\n",
       "              -0.0397  0.0106 -0.1339 -0.1931  0.2754  0.0009  0.2338  0.2199 -0.0022 -0.3260\n",
       "               0.2787  0.0726  0.0344  0.0211  0.1565 -0.3691  0.4593 -0.3364  0.0026 -0.1284\n",
       "               0.0128 -0.0174  0.1278 -0.1004 -0.1402 -0.0778 -0.0786  0.1124  0.1294 -0.1607\n",
       "              \n",
       "              Columns 30 to 39 \n",
       "              -0.1229 -0.0683 -0.0059  0.0243  0.1459  0.1685  0.0972 -0.0929  0.0479 -0.0935\n",
       "              -0.0777  0.0520 -0.1244 -0.0524 -0.1141 -0.0290 -0.1864  0.1523 -0.0784 -0.0161\n",
       "               0.1699  0.0389  0.1865  0.1244 -0.3608 -0.1640 -0.2823  0.0619 -0.0191 -0.0238\n",
       "               0.1855  0.0250  0.3936  0.1908  0.1120  0.0242  0.0889 -0.3342  0.2169 -0.4726\n",
       "              -0.1367 -0.2252 -0.3773 -0.1113  0.4088 -0.0941 -0.0674 -0.0634  0.0372 -0.1103\n",
       "              -0.2203 -0.3956 -0.4907 -0.3607  0.4288 -0.0187 -0.0946 -0.2651 -0.0357  0.4365\n",
       "              -0.1423 -0.0707 -0.4261 -0.0883 -0.2471  0.0245  0.0653  0.0760 -0.1702  0.0697\n",
       "              -0.0693 -0.0365  0.6106  0.2478 -0.1559  0.1691 -0.0591  0.0904 -0.1433  0.0980\n",
       "               0.3039 -0.2678  0.4713  0.0409  0.1157  0.0740  0.0661 -0.1494  0.0862  0.0997\n",
       "               0.4696 -0.0950  0.3135 -0.1193 -0.1117 -0.2275  0.0804 -0.0541  0.3130  0.1106\n",
       "              -0.0826 -0.0989 -0.1498 -0.0537  0.1231 -0.0034  0.0622 -0.1608  0.0514 -0.1685\n",
       "              \n",
       "              Columns 40 to 49 \n",
       "              -0.0491  0.0212  0.1411  0.1007  0.0135 -0.0806  0.0821  0.0553 -0.0586 -0.1485\n",
       "              -0.0186 -0.0914 -0.0389  0.0800  0.1288  0.0219 -0.0720 -0.1396  0.0716  0.0475\n",
       "              -0.3890 -0.1481  0.2183  0.1336  0.0687  0.0549  0.0197 -0.2891 -0.2563 -0.4524\n",
       "               0.3489  0.1689  0.1346 -0.5381 -0.2120 -0.0911  0.1465  0.0710  0.0908 -0.1365\n",
       "               0.6502 -0.1469  0.0068 -0.4894  0.0029  0.0385 -0.2588 -0.0697  0.2381  0.1441\n",
       "               0.2855 -0.0213 -0.0856  0.3450  0.0815  0.2050 -0.5279  0.3501 -0.0232  0.0207\n",
       "              -0.0414  0.3204  0.1423  0.2029  0.2587 -0.0558 -0.0009  0.1825  0.0357 -0.0559\n",
       "              -0.5764 -0.0259 -0.0697 -0.4580  0.1554  0.3052  0.1316  0.6184 -0.2782 -0.2287\n",
       "               0.1015 -0.2234 -0.0967  0.0014 -0.0139  0.0731 -0.1073 -0.1461 -0.1201 -0.1509\n",
       "              -0.0514  0.1066  0.1499 -0.3319 -0.3039 -0.0559 -0.3691  0.2065 -0.0711  0.0750\n",
       "              -0.0089  0.2506 -0.0352 -0.0383 -0.1350 -0.2619 -0.1019  0.2191  0.0423 -0.2092\n",
       "              [torch.FloatTensor of size 11x50]), ('linear.weight', \n",
       "               0.2647 -0.3162  0.1241  ...  -0.0934  0.1870  0.3497\n",
       "               0.1520 -0.2009  0.1082  ...  -0.0575  0.1953  0.4727\n",
       "               0.1111 -0.3540 -0.0397  ...  -0.2056  0.1919  0.2886\n",
       "                        ...             ⋱             ...          \n",
       "               0.2042 -0.2836  0.1495  ...  -0.0515  0.1497  0.2899\n",
       "               0.1362 -0.1494  0.1293  ...  -0.2552  0.0357  0.3692\n",
       "              -0.0859  0.1902 -0.0627  ...   0.0209 -0.3017  0.3848\n",
       "              [torch.FloatTensor of size 22x50]), ('linear.bias', \n",
       "              -0.1069\n",
       "              -0.1334\n",
       "              -0.1036\n",
       "              -0.1474\n",
       "               0.1215\n",
       "              -0.2371\n",
       "              -0.0349\n",
       "              -0.1745\n",
       "               0.2222\n",
       "              -0.0432\n",
       "              -0.2608\n",
       "               0.1352\n",
       "              -0.0533\n",
       "              -0.2204\n",
       "              -0.0629\n",
       "               0.2062\n",
       "               0.1510\n",
       "              -0.0690\n",
       "              -0.2674\n",
       "              -0.0023\n",
       "              -0.0359\n",
       "               0.1205\n",
       "              [torch.FloatTensor of size 22])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = E2EMN(VOCAB_SIZE, EMBED_SIZE, n_hops=N_HOPS, encoding_method=ENCODING_METHOD, \n",
    "              temporal=TEMPORAL, use_cuda=USE_CUDA, max_story_len=MAX_STORY_LEN)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# if USE_CUDA:\n",
    "#     model = model.cuda()\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "# else:\n",
    "#     pass\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed_A.weight', \n",
       "               8.7212e-02 -2.4311e-02 -5.3458e-02  ...  -1.0072e-01 -7.0019e-02 -8.0092e-02\n",
       "              -1.2276e-01  8.6810e-03  5.4344e-02  ...   1.3872e-01 -1.1800e-03  6.2671e-02\n",
       "              -1.4740e-01  1.6852e-01 -1.3936e-01  ...   1.6313e-01  9.0611e-02 -4.2169e-01\n",
       "                              ...                   ⋱                   ...                \n",
       "              -2.3257e-01 -1.2822e-01  5.6160e-01  ...   2.9820e-01  1.0062e-01  1.2733e-01\n",
       "               2.0542e-02 -6.0076e-02 -2.2000e-02  ...   2.1464e-02  4.2375e-02  7.6490e-02\n",
       "              -1.7213e-05  8.0307e-02 -1.4132e-02  ...  -3.1378e-02 -1.9798e-01 -4.4935e-02\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_B.weight', \n",
       "              -0.0114 -0.0028  0.0633  ...  -0.0429  0.1092  0.1136\n",
       "               0.0152 -0.1037 -0.2155  ...  -0.0356 -0.0099 -0.1166\n",
       "              -0.0591  0.0252  0.1548  ...  -0.0778  0.0614  0.0145\n",
       "                        ...             ⋱             ...          \n",
       "              -0.4203 -0.2356  0.2772  ...   0.2522  0.1150  0.0101\n",
       "              -0.1467  0.2086 -0.0034  ...   0.2003  0.1051 -0.0217\n",
       "              -0.1650  0.0674  0.0208  ...   0.0020 -0.0461 -0.0029\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_C.weight', \n",
       "              -0.1772 -0.0378  0.0515  ...   0.2341 -0.1668 -0.0509\n",
       "              -0.2220 -0.1890  0.0266  ...   0.1400 -0.1312  0.0242\n",
       "              -0.1204  0.1516 -0.1542  ...   0.0787 -0.2124 -0.0711\n",
       "                        ...             ⋱             ...          \n",
       "              -0.0731  0.0177  0.2357  ...   0.0482 -0.1711 -0.1522\n",
       "               0.1831  0.0105  0.0738  ...  -0.0102  0.0692 -0.1884\n",
       "               0.2193  0.9664 -0.5530  ...  -0.3732 -0.8419  0.9935\n",
       "              [torch.FloatTensor of size 22x50]), ('embed_A_T.weight', \n",
       "              \n",
       "              Columns 0 to 9 \n",
       "              -0.0588 -0.0688 -0.0664 -0.0541  0.2300  0.0692 -0.0114 -0.1658  0.0815  0.0111\n",
       "               0.4951 -0.3592  0.1473  0.3761  0.0944 -0.1153 -0.0215  0.1104  0.0844  0.3843\n",
       "               0.1770 -0.2283 -0.0100  0.0789 -0.1066  0.1780  0.0416  0.0429  0.3036  0.1992\n",
       "              -0.0453 -0.2125  0.2530  0.0144  0.0800  0.1098 -0.1930  0.0598  0.3144  0.1533\n",
       "               0.0882 -0.0942  0.1722 -0.0319  0.0292  0.0786 -0.1632 -0.2797 -0.0644  0.2717\n",
       "               0.2738  0.0802  0.1392  0.2070  0.0719  0.0195 -0.0380 -0.1765 -0.2330  0.0024\n",
       "              -0.1017  0.2673 -0.3322 -0.0220 -0.2454 -0.1694 -0.0324  0.1177 -0.1571 -0.0816\n",
       "              -0.2166  0.3913 -0.4236 -0.0028 -0.1201 -0.0120  0.2311  0.1433 -0.1676 -0.2430\n",
       "              -0.3892  0.0989 -0.3070 -0.1489  0.0476 -0.1538  0.4739 -0.1177 -0.1213 -0.2030\n",
       "              -0.1321  0.2295 -0.1497 -0.3349 -0.0104 -0.2844  0.2849 -0.1953 -0.4430 -0.3144\n",
       "              -0.4900  0.4258 -0.3665 -0.5024  0.0985 -0.2033  0.3065  0.1422 -0.2476 -0.8226\n",
       "              \n",
       "              Columns 10 to 19 \n",
       "              -0.1997 -0.0293  0.1595 -0.0263  0.0486 -0.1024  0.0698 -0.0040 -0.1500  0.0681\n",
       "              -0.0920 -0.3544 -0.0224  0.1585  0.3361 -0.3899  0.3863 -0.3251 -0.1247  0.5929\n",
       "               0.1923 -0.2080 -0.0529  0.1220  0.1999 -0.2145  0.3563 -0.2784 -0.0435  0.3388\n",
       "               0.1791  0.0073 -0.0068  0.1019  0.0570 -0.2660  0.0119 -0.1284 -0.0861  0.2046\n",
       "               0.1317  0.0928  0.2691 -0.0852  0.0363  0.1299  0.0099 -0.1191 -0.0932  0.0061\n",
       "              -0.1547  0.0305  0.1129  0.1343 -0.0335 -0.1197 -0.2192  0.1813  0.0356 -0.1240\n",
       "               0.3325  0.1823 -0.0309 -0.0624  0.0623  0.1642 -0.0630 -0.1476  0.3401 -0.1219\n",
       "               0.2772  0.2446  0.1355 -0.0059 -0.1863  0.2879 -0.1398 -0.0657  0.2895 -0.1088\n",
       "               0.0540  0.2366  0.0656 -0.0385 -0.3467  0.2836 -0.1423 -0.1369 -0.1701 -0.2332\n",
       "              -0.1773  0.4941  0.0311 -0.1387 -0.3348  0.3187 -0.2383  0.5089  0.2002 -0.4756\n",
       "              -0.1232  0.3716 -0.3475 -0.0353 -0.1392  0.5692 -0.2267  0.1290  0.3928 -0.5044\n",
       "              \n",
       "              Columns 20 to 29 \n",
       "               0.0932 -0.0183  0.0185  0.1330 -0.3078  0.2236 -0.1047 -0.1347  0.0451 -0.0960\n",
       "              -0.5329  0.2923  0.0585  0.1817  0.0123  0.0124 -0.4210  0.1674  0.4039 -0.1659\n",
       "              -0.0365  0.1712 -0.0665  0.3987 -0.0566 -0.0296 -0.0851  0.3213  0.2117 -0.0091\n",
       "              -0.0042  0.2495 -0.0721  0.0011  0.0513 -0.1976  0.3409 -0.3340  0.1475  0.2234\n",
       "              -0.0616 -0.1080  0.1009 -0.3681 -0.0045  0.1140 -0.0800 -0.1760 -0.0166  0.1005\n",
       "               0.1559 -0.1814  0.1801 -0.1056  0.0604  0.0815 -0.0974 -0.2331 -0.0973 -0.1163\n",
       "               0.1498 -0.1464  0.1865  0.0209 -0.1248  0.0531  0.1329  0.1678  0.0108 -0.2048\n",
       "               0.3504 -0.1893  0.2918 -0.1057 -0.0276 -0.0643 -0.0664  0.0276 -0.0429  0.0969\n",
       "               0.4167 -0.2782  0.3058 -0.5401  0.1301 -0.0828  0.0788  0.2760 -0.1867 -0.2010\n",
       "               0.5549 -0.2057  0.0966 -0.2059  0.4710 -0.2069  0.4855 -0.1106 -0.1620 -0.0641\n",
       "               0.5793 -0.3850  0.1584 -0.4030  0.1358 -0.4313  0.4784  0.0080 -0.1740 -0.2456\n",
       "              \n",
       "              Columns 30 to 39 \n",
       "               0.0564  0.0419 -0.0365  0.0374 -0.0154 -0.0770  0.0004  0.0232  0.0716 -0.0161\n",
       "              -0.2836  0.2343 -0.3269  0.0779 -0.1950  0.2385 -0.2775  0.5109 -0.5399 -0.3524\n",
       "              -0.2595  0.1615 -0.2134 -0.1791 -0.2308 -0.0281 -0.4392  0.4897 -0.3567  0.0526\n",
       "              -0.0392  0.1243  0.2408  0.1987 -0.2276  0.0092 -0.1612  0.1368 -0.1398 -0.1863\n",
       "              -0.0329  0.1521  0.1777  0.0003 -0.0216  0.0210 -0.0478  0.0892 -0.0301 -0.3777\n",
       "              -0.0994 -0.1009 -0.0011 -0.1358  0.2820 -0.0312 -0.0913 -0.1236  0.1106 -0.1213\n",
       "              -0.2222 -0.1131 -0.2496 -0.2683  0.2616 -0.0359  0.2061 -0.2099  0.0420  0.1009\n",
       "              -0.1788 -0.1326  0.0424 -0.2426  0.2152  0.0473  0.2124 -0.2250  0.0468  0.1102\n",
       "              -0.2066 -0.1295  0.3110  0.0946  0.3181  0.0968  0.3913 -0.3912  0.2346  0.3136\n",
       "               0.3362 -0.2936  0.4817 -0.1671  0.3492 -0.0968  0.3533 -0.4362  0.3492  0.4397\n",
       "               0.3786 -0.3016  0.3834 -0.2202  0.4405 -0.2165  0.3168 -0.6214  0.6713  0.2891\n",
       "              \n",
       "              Columns 40 to 49 \n",
       "               0.0666  0.0234  0.0113  0.0455  0.1141 -0.0157  0.0189 -0.1354 -0.0167  0.0114\n",
       "               0.3667 -0.1619  0.3488  0.4821 -0.1563 -0.0307  0.4092 -0.4442  0.2327  0.5301\n",
       "              -0.1771 -0.2407  0.1591  0.4240  0.0876 -0.0657  0.1274 -0.3697 -0.0864  0.0728\n",
       "              -0.2988 -0.1188  0.2258  0.3697 -0.0572  0.1313  0.1659 -0.3249 -0.0725 -0.0826\n",
       "               0.0922 -0.0657  0.0522 -0.1166 -0.1925 -0.1269  0.1736 -0.1764  0.0787 -0.2014\n",
       "               0.3879 -0.2116 -0.0437 -0.2143 -0.0056  0.1265 -0.1745 -0.1272 -0.0790  0.0012\n",
       "               0.2218  0.1895 -0.0968 -0.0095  0.0748  0.1454 -0.2297  0.0393  0.1353 -0.1435\n",
       "              -0.0796  0.4546 -0.2188 -0.3094  0.3743 -0.0341 -0.3211  0.3198 -0.1762 -0.1655\n",
       "              -0.3294  0.2475 -0.3689 -0.3941  0.3252  0.3834 -0.3192  0.3548 -0.3149 -0.5185\n",
       "              -0.2034  0.1065 -0.4359 -0.5217  0.2701  0.3499 -0.3392  0.3339 -0.3226 -0.4167\n",
       "              -0.2967  0.5313 -0.6216 -0.4606  0.0782  0.1327 -0.5646  0.5114 -0.2761 -0.6384\n",
       "              [torch.FloatTensor of size 11x50]), ('embed_C_T.weight', \n",
       "              \n",
       "              Columns 0 to 9 \n",
       "               0.0288  0.0499 -0.0834 -0.0649  0.0644  0.1646  0.1544 -0.0558 -0.0002 -0.0200\n",
       "              -0.2045 -0.1601 -0.0073  0.1596 -0.0815  0.2384  0.1204 -0.0597  0.0609 -0.2080\n",
       "              -0.1891 -0.0128  0.2653 -0.1137  0.1117  0.0840 -0.1884  0.2661  0.4693  0.0487\n",
       "              -0.0526  0.1441  0.3619  0.0947  0.3413 -0.0683 -0.1006 -0.2511 -0.4082  0.1239\n",
       "               0.4442  0.3713 -0.0095  0.1978  0.0207 -0.1729 -0.0188 -0.0892 -0.4551  0.1195\n",
       "              -0.1715  0.4341 -0.3666  0.0605 -0.2256 -0.2228  0.2314  0.2962 -0.2640 -0.2922\n",
       "              -0.4057  0.1776 -0.4001  0.0127 -0.2375 -0.0128  0.1054  0.2212  0.2729  0.2042\n",
       "              -0.3559 -0.3081  0.0324 -0.0260  0.3801  0.0579  0.6589 -0.2140  0.0878  0.0697\n",
       "              -0.1260 -0.3421  0.3757 -0.3130  0.2886 -0.1978  0.1849 -0.1668  0.0080  0.0403\n",
       "              -0.2115  0.1146  0.0492 -0.3963  0.0280 -0.1753 -0.1225  0.1283 -0.1148 -0.4282\n",
       "              -0.3024  0.1277 -0.3434  0.0098  0.2385  0.1171  0.2174  0.3149  0.3507 -0.4088\n",
       "              \n",
       "              Columns 10 to 19 \n",
       "              -0.0327 -0.0245  0.0876 -0.0670  0.0493 -0.1325  0.0583  0.1315 -0.0307 -0.0070\n",
       "               0.2108 -0.0136 -0.2877  0.1939  0.0813  0.1409 -0.0039 -0.0496  0.1705  0.1027\n",
       "               0.0967  0.0443  0.2206 -0.0226 -0.1113 -0.1244 -0.0624 -0.0231 -0.1981 -0.0346\n",
       "              -0.0995  0.2867  0.3574 -0.0365 -0.2269  0.1091 -0.3567  0.1539 -0.0648 -0.1166\n",
       "              -0.0601  0.0940  0.0749  0.1369 -0.0095  0.0981 -0.2166  0.1241  0.0874 -0.1812\n",
       "               0.0180  0.1639 -0.3372  0.2948  0.0593 -0.0738 -0.1120  0.1792  0.3867  0.0947\n",
       "               0.5082  0.0723  0.0147 -0.1278 -0.1375  0.2164  0.2984 -0.3755  0.0959  0.1535\n",
       "              -0.2769 -0.0504  0.1296  0.0877 -0.5519 -0.1329  0.1568 -0.0891 -0.6430  0.1782\n",
       "              -0.2380  0.1716 -0.2209 -0.0336 -0.1324  0.1227 -0.1074  0.2493 -0.1747 -0.1558\n",
       "              -0.2274  0.3234 -0.4533  0.1599  0.1739  0.1474 -0.1683  0.3166  0.4361 -0.2296\n",
       "               0.1934 -0.2954 -0.4371  0.1506  0.0992  0.0607 -0.1460 -0.3094 -0.0816 -0.0729\n",
       "              \n",
       "              Columns 20 to 29 \n",
       "              -0.1323  0.1378 -0.0537  0.1608  0.0823  0.0005  0.0726  0.1413  0.0220  0.0475\n",
       "               0.0823 -0.0059  0.0175  0.0217  0.0984  0.1390  0.0015  0.1602  0.0304 -0.1291\n",
       "               0.0443  0.2735 -0.2336  0.0697 -0.0070 -0.1531  0.4155 -0.2834 -0.0294  0.1468\n",
       "              -0.0513 -0.2861  0.0939 -0.5902  0.0125  0.2282 -0.1232 -0.4668 -0.0458  0.1614\n",
       "              -0.1040 -0.1404  0.1713  0.0496  0.0715  0.1700 -0.6452 -0.1369 -0.0732 -0.3125\n",
       "               0.1679  0.0253  0.0663  0.3221 -0.0683  0.0281  0.0618  0.1092 -0.0606 -0.2967\n",
       "              -0.2213  0.0690  0.1949  0.1376 -0.2565  0.1861 -0.1890  0.1549  0.2419  0.0561\n",
       "               0.1912 -0.0295  0.1962 -0.3753  0.1277  0.1394 -0.3867 -0.0130 -0.0640 -0.0174\n",
       "              -0.0397  0.0106 -0.1339 -0.1931  0.2754  0.0009  0.2338  0.2199 -0.0022 -0.3260\n",
       "               0.2787  0.0726  0.0344  0.0211  0.1565 -0.3691  0.4593 -0.3364  0.0026 -0.1284\n",
       "               0.0128 -0.0174  0.1278 -0.1004 -0.1402 -0.0778 -0.0786  0.1124  0.1294 -0.1607\n",
       "              \n",
       "              Columns 30 to 39 \n",
       "              -0.1229 -0.0683 -0.0059  0.0243  0.1459  0.1685  0.0972 -0.0929  0.0479 -0.0935\n",
       "              -0.0777  0.0520 -0.1244 -0.0524 -0.1141 -0.0290 -0.1864  0.1523 -0.0784 -0.0161\n",
       "               0.1699  0.0389  0.1865  0.1244 -0.3608 -0.1640 -0.2823  0.0619 -0.0191 -0.0238\n",
       "               0.1855  0.0250  0.3936  0.1908  0.1120  0.0242  0.0889 -0.3342  0.2169 -0.4726\n",
       "              -0.1367 -0.2252 -0.3773 -0.1113  0.4088 -0.0941 -0.0674 -0.0634  0.0372 -0.1103\n",
       "              -0.2203 -0.3956 -0.4907 -0.3607  0.4288 -0.0187 -0.0946 -0.2651 -0.0357  0.4365\n",
       "              -0.1423 -0.0707 -0.4261 -0.0883 -0.2471  0.0245  0.0653  0.0760 -0.1702  0.0697\n",
       "              -0.0693 -0.0365  0.6106  0.2478 -0.1559  0.1691 -0.0591  0.0904 -0.1433  0.0980\n",
       "               0.3039 -0.2678  0.4713  0.0409  0.1157  0.0740  0.0661 -0.1494  0.0862  0.0997\n",
       "               0.4696 -0.0950  0.3135 -0.1193 -0.1117 -0.2275  0.0804 -0.0541  0.3130  0.1106\n",
       "              -0.0826 -0.0989 -0.1498 -0.0537  0.1231 -0.0034  0.0622 -0.1608  0.0514 -0.1685\n",
       "              \n",
       "              Columns 40 to 49 \n",
       "              -0.0491  0.0212  0.1411  0.1007  0.0135 -0.0806  0.0821  0.0553 -0.0586 -0.1485\n",
       "              -0.0186 -0.0914 -0.0389  0.0800  0.1288  0.0219 -0.0720 -0.1396  0.0716  0.0475\n",
       "              -0.3890 -0.1481  0.2183  0.1336  0.0687  0.0549  0.0197 -0.2891 -0.2563 -0.4524\n",
       "               0.3489  0.1689  0.1346 -0.5381 -0.2120 -0.0911  0.1465  0.0710  0.0908 -0.1365\n",
       "               0.6502 -0.1469  0.0068 -0.4894  0.0029  0.0385 -0.2588 -0.0697  0.2381  0.1441\n",
       "               0.2855 -0.0213 -0.0856  0.3450  0.0815  0.2050 -0.5279  0.3501 -0.0232  0.0207\n",
       "              -0.0414  0.3204  0.1423  0.2029  0.2587 -0.0558 -0.0009  0.1825  0.0357 -0.0559\n",
       "              -0.5764 -0.0259 -0.0697 -0.4580  0.1554  0.3052  0.1316  0.6184 -0.2782 -0.2287\n",
       "               0.1015 -0.2234 -0.0967  0.0014 -0.0139  0.0731 -0.1073 -0.1461 -0.1201 -0.1509\n",
       "              -0.0514  0.1066  0.1499 -0.3319 -0.3039 -0.0559 -0.3691  0.2065 -0.0711  0.0750\n",
       "              -0.0089  0.2506 -0.0352 -0.0383 -0.1350 -0.2619 -0.1019  0.2191  0.0423 -0.2092\n",
       "              [torch.FloatTensor of size 11x50]), ('linear.weight', \n",
       "               0.2647 -0.3162  0.1241  ...  -0.0934  0.1870  0.3497\n",
       "               0.1520 -0.2009  0.1082  ...  -0.0575  0.1953  0.4727\n",
       "               0.1111 -0.3540 -0.0397  ...  -0.2056  0.1919  0.2886\n",
       "                        ...             ⋱             ...          \n",
       "               0.2042 -0.2836  0.1495  ...  -0.0515  0.1497  0.2899\n",
       "               0.1362 -0.1494  0.1293  ...  -0.2552  0.0357  0.3692\n",
       "              -0.0859  0.1902 -0.0627  ...   0.0209 -0.3017  0.3848\n",
       "              [torch.FloatTensor of size 22x50]), ('linear.bias', \n",
       "              -0.1069\n",
       "              -0.1334\n",
       "              -0.1036\n",
       "              -0.1474\n",
       "               0.1215\n",
       "              -0.2371\n",
       "              -0.0349\n",
       "              -0.1745\n",
       "               0.2222\n",
       "              -0.0432\n",
       "              -0.2608\n",
       "               0.1352\n",
       "              -0.0533\n",
       "              -0.2204\n",
       "              -0.0629\n",
       "               0.2062\n",
       "               0.1510\n",
       "              -0.0690\n",
       "              -0.2674\n",
       "              -0.0023\n",
       "              -0.0359\n",
       "               0.1205\n",
       "              [torch.FloatTensor of size 22])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_test = '../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_test.txt'\n",
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)\n",
    "test_loader = bAbIDataLoader(dataset=bAbI_test, batch_size=32, shuffle=False, to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "막 training 끝마치고 나서 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "    \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나갔다가 load state 하고 나온 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.181\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for i, batch in enumerate(test_loader.load()):\n",
    "    stories, stories_masks, questions, _, answers, _ = batch\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        stories = [get_cuda(x) for x in stories]\n",
    "        stories_masks = [get_cuda(x) for x in stories_masks]\n",
    "        questions, answers = get_cuda(questions, answers)\n",
    "    \n",
    "    for story, mask, q, a in zip(stories, stories_masks, questions, answers):\n",
    "        model.zero_grad()\n",
    "        pred = model(story.unsqueeze(0), q.unsqueeze(0), stories_masks=mask.unsqueeze(0))\n",
    "        accuracy += torch.eq(torch.max(pred, 1)[1], a).data[0]\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy/len(bAbI_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic: 0.653\n",
    "* pe_te: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bAbI_test = bAbIDataset(path_test, train=False, vocab=bAbI_train.word2idx, return_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3, 7, 15, 16, 10],\n",
       "  [3, 17, 2, 15, 16, 6],\n",
       "  [14, 7, 15, 16, 6],\n",
       "  [19, 17, 2, 15, 16, 10],\n",
       "  [14, 17, 15, 16, 10],\n",
       "  [19, 5, 15, 16, 6],\n",
       "  [3, 13, 15, 16, 10],\n",
       "  [3, 13, 15, 16, 6]],\n",
       " [18, 20, 3, 21],\n",
       " [6],\n",
       " 11]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(bAbI_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "story, q, a, s = bAbI_test.pad_to_story([random.choice(bAbI_test.data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: random print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "story, q, a, s = bAbI_test.pad_to_story([random.choice(bAbI_test.data)], w2idx)\n",
    "model.zero_grad()\n",
    "pred = model(story[0].unsqueeze(0), q)\n",
    "pred_a = torch.max(pred, 1)[1]\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('-'*45)\n",
    "print('\\n'.join([' '.join(list(map(lambda x: idx2w[x], f))) for f in story[0].data.tolist()]))\n",
    "print('-'*45)\n",
    "print(\"Question : \",' '.join(list(map(lambda x: idx2w[x], q.data.tolist()[0]))))\n",
    "print('-'*45)\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: idx2w[x], a.squeeze(1).data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: idx2w[x], pred_a.data.tolist()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
