{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from bAbI_data_utils import bAbI_data_load, data_loader, pad_to_batch, pad_to_fact\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to data loading...\n"
     ]
    }
   ],
   "source": [
    "train_data, word2index = bAbI_data_load(\"../data/QA_bAbI_tasks/en-10k/qa1_single-supporting-fact_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = data_loader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "facts, fact_masks, questions, question_masks, answers = pad_to_batch(batch, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figs/E2EMN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer\n",
    "Sentences: \n",
    "\n",
    "$$X = [x_1, x_2, \\cdots, x_n]: n \\times T_c$$\n",
    "\n",
    "* $n$: number of sentences in context\n",
    "* $T_c$: max length of a sentence in context\n",
    "\n",
    "Embeding Matrix: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &: d \\times V \\\\\n",
    "B &: d \\times V \\\\\n",
    "C &: d \\times V\n",
    "\\end{aligned}$$\n",
    "\n",
    "inputs: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_i &: T_c \\times d \\\\ \n",
    "u &: T_q \\times d\n",
    "\\end{aligned}$$\n",
    "\n",
    "total embedding of context: $M : n \\times T_c \\times d$\n",
    "* $m_i$: summation embedded for each sentence in context as length of $T_c$, $n \\times d$\n",
    "* $u$: summation embedded for query(question) as length of $T_q$, $1 \\times d$\n",
    "* $score = m_iu^T: (n \\times d) \\cdot (d \\times 1) = n \\times 1$\n",
    "\n",
    "attention:\n",
    "$$\\begin{aligned}\n",
    "p &= softmax(score): n \\times 1 \\\\\n",
    "o &= c^T p : d \\times 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "summation vectors to linear layer:\n",
    "$$\\begin{aligned}\n",
    "inputs = u + o : d \\times 1 \\\\\n",
    "a = softmax(W \\cdot inputs) : (V \\times d) \\times (d \\times 1) = V \\times 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1503.08895.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    8     6    17    19    23     3     0\n",
       "   22    21    15    17    19     7     3\n",
       "    8    13    17    19    18     3     0\n",
       "    8    12    17    19     7     3     0\n",
       "    0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0\n",
       "[torch.LongTensor of size 10x7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts[0]  # n, T_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 16\n",
       " 10\n",
       "  8\n",
       " 14\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0] # T_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 5\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = nn.Embedding(len(word2index), d, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Embedding(24, 5, padding_idx=0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_a = embed(facts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_b = embed(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_b.size()  # T_q, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_a.size()  # n, T_c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.4360  5.4795  2.6442  1.1397 -2.2188\n",
       "-5.3981  3.0593 -0.1660  0.2619 -1.0017\n",
       "-5.2118  2.7386  1.0787 -1.1378 -2.8889\n",
       "-4.8005 -0.0609  0.0309 -2.9232 -0.8567\n",
       "-5.3981  3.0593 -0.1660  0.2619 -1.0017\n",
       "-4.8060  1.8243 -3.2936 -1.6170  0.2388\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 10x5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = embed_a.sum(dim=1) # n, d\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.4348\n",
       " 3.1010\n",
       "-2.8605\n",
       " 3.2210\n",
       "-1.8064\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = embed_b.sum(dim=0) # d\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 18.6007\n",
       " 14.9615\n",
       "  9.2263\n",
       " -6.0585\n",
       " 14.9615\n",
       " 11.5281\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "[torch.FloatTensor of size 10x1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = torch.mm(m, u.unsqueeze(1)) # n, 1\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 9.4923e-01\n",
       " 2.4941e-02\n",
       " 8.0560e-05\n",
       " 1.8537e-11\n",
       " 2.4941e-02\n",
       " 8.0498e-04\n",
       " 7.9288e-09\n",
       " 7.9288e-09\n",
       " 7.9288e-09\n",
       " 7.9288e-09\n",
       "[torch.FloatTensor of size 10x1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = F.softmax(score, dim=0) # n, 1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_c = embed(facts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = embed_c.sum(1) # n, d\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.5351\n",
       " 5.3556\n",
       " 2.4991\n",
       " 1.0935\n",
       "-2.1562\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = torch.mm(c.t(), p) # d, 1\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = u.unsqueeze(1) + o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.9699\n",
       " 8.4565\n",
       "-0.3614\n",
       " 4.3145\n",
       "-3.9626\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, mem_size):\n",
    "        super(E2EMN, self).__init__()\n",
    "        \n",
    "        # sharing matrix for k hops\n",
    "        self.embed_A = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_B = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_C = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
