{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import torch.nn.init as init\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(fname, word2idx, max_words, max_sentences):\n",
    "    # stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "    # questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "    stories = dict()\n",
    "    questions = dict()\n",
    "    \n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<null>'] = 0\n",
    "\n",
    "    \n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname) as f:\n",
    "            lines = f.readlines()\n",
    "    else:\n",
    "        raise Exception(\"[!] Data {file} not found\".format(file=fname))\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        max_words = max(max_words, len(words))\n",
    "        \n",
    "        # Determine whether the line indicates the start of a new story\n",
    "        if words[0] == '1':\n",
    "            story_ind = len(stories)\n",
    "            sentence_ind = 0\n",
    "            stories[story_ind] = []\n",
    "        \n",
    "        # Determine whether the line is a question or not\n",
    "        if '?' in line:\n",
    "            is_question = True\n",
    "            question_ind = len(questions)\n",
    "            questions[question_ind] = {'question': [], 'answer': [], 'story_index': story_ind, 'sentence_index': sentence_ind}\n",
    "        else:\n",
    "            is_question = False\n",
    "            sentence_ind = len(stories[story_ind])\n",
    "        \n",
    "        # Parse and append the words to appropriate dictionary / Expand word2idx dictionary\n",
    "        sentence_list = []\n",
    "        for k in range(1, len(words)):\n",
    "            w = words[k].lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            if ('.' in w) or ('?' in w):\n",
    "                w = w[:-1]\n",
    "            \n",
    "            # Add new word to dictionary\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = len(word2idx)\n",
    "            \n",
    "            # Append sentence to story dict if not question\n",
    "            if not is_question:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '.' in words[k]:\n",
    "                    stories[story_ind].append(sentence_list)\n",
    "                    break\n",
    "            \n",
    "            # Append sentence and answer to question dict if question\n",
    "            else:\n",
    "                sentence_list.append(w)\n",
    "                \n",
    "                if '?' in words[k]:\n",
    "                    answer = words[k + 1].lower()\n",
    "                    \n",
    "                    if answer not in word2idx:\n",
    "                        word2idx[answer] = len(word2idx)\n",
    "                    \n",
    "                    questions[question_ind]['question'].extend(sentence_list)\n",
    "                    questions[question_ind]['answer'].append(answer)\n",
    "                    break\n",
    "        \n",
    "        # Update max_sentences\n",
    "        max_sentences = max(max_sentences, sentence_ind)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the words into indices\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            temp = list(map(word2idx.get, context[i]))\n",
    "            context[i] = temp\n",
    "    \n",
    "    for idx, value in questions.items():\n",
    "        temp1 = list(map(word2idx.get, value['question']))\n",
    "        temp2 = list(map(word2idx.get, value['answer']))\n",
    "        \n",
    "        value['question'] = temp1\n",
    "        value['answer'] = temp2\n",
    "    \n",
    "    return stories, questions, max_words, max_sentences\n",
    "\n",
    "\n",
    "def pad_data(stories, questions, max_words, max_sentences):\n",
    "\n",
    "    # Pad the context into same size with '<null>'\n",
    "    for idx, context in stories.items():\n",
    "        for sentence in context:           \n",
    "            while len(sentence) < max_words:\n",
    "                sentence.append(0)\n",
    "        while len(context) < max_sentences:\n",
    "            context.append([0] * max_words)\n",
    "    \n",
    "    # Pad the question into same size with '<null>'\n",
    "    for idx, value in questions.items():\n",
    "        while len(value['question']) < max_words:\n",
    "            value['question'].append(0)\n",
    "\n",
    "\n",
    "def depad_data(stories, questions):\n",
    "\n",
    "    for idx, context in stories.items():\n",
    "        for i in range(len(context)):\n",
    "            if 0 in context[i]:\n",
    "                if context[i][0] == 0:\n",
    "                    temp = context[:i]\n",
    "                    context = temp\n",
    "                    break\n",
    "                else:\n",
    "                    index = context[i].index(0)\n",
    "                    context[i] = context[i][:index]\n",
    "\n",
    "    for idx, value in questions.items():\n",
    "        if 0 in value['question']:\n",
    "            index = value['question'].index(0)\n",
    "            value['question'] = value['question'][:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stories[story_ind] = [[sentence1], [sentence2], ..., [sentenceN]]\n",
    "# questions[question_ind] = {'question': [question], 'answer': [answer], 'story_index': #, 'sentence_index': #}\n",
    "data_path = \"./data/Memory_End_to-End_Network_Project/bAbi-tasks/QA_bAbI_tasks/en/\"\n",
    "word2idx = {}\n",
    "max_words = 0\n",
    "max_sentences = 0\n",
    "train_fname = data_path + \"qa1_single-supporting-fact_train.txt\"\n",
    "test_fname = data_path + \"qa1_single-supporting-fact_test.txt\"\n",
    "\n",
    "train_stories, train_questions, max_words, max_sentences = \\\n",
    "    read_data(train_fname, word2idx, max_words, max_sentences)\n",
    "test_stories, test_questions, max_words, max_sentences = \\\n",
    "    read_data(test_fname, word2idx, max_words, max_sentences)\n",
    "    \n",
    "pad_data(train_stories, train_questions, max_words, max_sentences)\n",
    "pad_data(test_stories, test_questions, max_words, max_sentences)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "\n",
    "nwords = len(word2idx)\n",
    "\n",
    "config = defaultdict()\n",
    "config['vocab_size'] = nwords\n",
    "config['max_words'] = max_words\n",
    "config['embedding_size'] = 30\n",
    "config['n_hops'] = 3\n",
    "# config['mem_size'] = 50\n",
    "config['batch_size'] = 32\n",
    "config['nepoch'] = 100\n",
    "config['anneal_epoch'] = 25  # anneal the learning rate every <anneal_epoch> epochs\n",
    "config['babi_task'] = 1  # index of bAbI task for the network to learn\n",
    "config['init_lr'] = 0.01\n",
    "config['anneal_rate'] = 0.5\n",
    "config['init_mean'] = 0.0\n",
    "config['init_std'] = 0.1\n",
    "config['max_grad_norm'] = 40  # clip gradients to this norm\n",
    "config['lin_start'] = False  # True for linear start training, False for otherwise\n",
    "config['is_test'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def position_encoding(sentence_size, embedding_dim):\n",
    "#     encoding = np.ones((embedding_dim, sentence_size), dtype=np.float32)\n",
    "#     ls = sentence_size + 1\n",
    "#     le = embedding_dim + 1\n",
    "#     for i in range(1, le):\n",
    "#         for j in range(1, ls):\n",
    "#             encoding[i-1, j-1] = (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)\n",
    "#     encoding = 1 + 4 * encoding / embedding_dim / sentence_size\n",
    "#     # Make position encoding of time words identity to avoid modifying them\n",
    "#     encoding[:, -1] = 1.0\n",
    "#     return np.transpose(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttrProxy(object):\n",
    "    \"\"\"\n",
    "    Translates index lookups into attribute lookups.\n",
    "    To implement some trick which able to use list of nn.Module in a nn.Module\n",
    "    see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n",
    "    \"\"\"\n",
    "    def __init__(self, module, prefix):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return getattr(self.module, self.prefix + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class E2EMN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(E2EMN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.n_hops = config['n_hops']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nepoch = config['nepoch']\n",
    "        self.anneal_epoch = config['anneal_epoch']\n",
    "        self.babi_task = config['babi_task']\n",
    "        self.init_lr = config['init_lr']\n",
    "        self.anneal_rate = config['anneal_rate']\n",
    "        self.init_mean = config['init_mean']\n",
    "        self.init_std = config['init_std']\n",
    "        self.max_grad_norm = config['max_grad_norm']\n",
    "        self.lin_start = config['lin_start']\n",
    "        self.is_test = config['is_test']\n",
    "        self.sentence_size = config['max_words']\n",
    "        \n",
    "        for hop in range(self.n_hops+1):\n",
    "            C = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=0)\n",
    "            C.weight.data.normal_(0, 0.1)\n",
    "            self.add_module('C_{}'.format(hop), C)\n",
    "        self.C = AttrProxy(self, \"C_\")\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.softmax = nn.Softmax(dim=0)            \n",
    "        self.encoding = Variable(torch.FloatTensor(\n",
    "            (self.sentence_size, self.embedding_size)), requires_grad=True)\n",
    "                                                  \n",
    "            \n",
    "    def forward(self, story, query):\n",
    "        \n",
    "        u = []\n",
    "        query_embed = self.C[0](query)\n",
    "        \n",
    "        u.append(query_embed.unsqueeze(0).sum(dim=1))  # u (1, embedding_size)\n",
    "\n",
    "        for hop in range(self.n_hops):\n",
    "            \n",
    "            embed_A = self.C[hop](story)  # story_size, sentence_size, embedding_size\n",
    "            embed_A = embed_A.view()\n",
    "            print(embed_A)\n",
    "            embed_A = self.encoding(embed_A)\n",
    "            print(embed_A.shape)\n",
    "            m_A = torch.sum(embed_A, 1)  # story_size, embedding_size\n",
    "            prob = self.softmax(m_A @ u[-1].t())\n",
    "            \n",
    "            embed_C = self.C[hop](story)  # story_size, sentence_size, embedding_size\n",
    "            m_C = torch.sum(embed_C, 1)  # story_size, embedding_size\n",
    "            \n",
    "            o_k = torch.sum(m_C*prob, 0).unsqueeze(0)  # 1, embedding_size\n",
    "            \n",
    "            u_k = u[-1] + o_k\n",
    "            \n",
    "        a_hat = self.C[self.n_hops].weight @ u[-1].t()  # vocab_size, 1\n",
    "        return a_hat.t(), self.softmax(a_hat).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem_network = E2EMN(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (story, query, answer) in making_data_set(train_stories, train_questions):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_A = mem_network.encoder(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -7.54053831e-01,  -4.90354300e-01,   3.67807090e-01,\n",
       "           1.88888717e+00,   1.31979418e+00,  -4.67547089e-01,\n",
       "           1.57792962e+00,  -1.18782997e+00,   5.67408860e-01,\n",
       "          -3.37423861e-01,   1.14038646e+00,   1.50130916e+00,\n",
       "          -9.51272488e-01,   9.33103681e-01,   9.21080470e-01,\n",
       "           1.26116204e+00,  -2.43288890e-01,  -2.59721041e-01,\n",
       "           2.84962088e-01,   9.28212523e-01,   1.76599705e+00,\n",
       "           5.34330234e-02,  -9.47482586e-02,   1.00768852e+00,\n",
       "           5.00619531e-01,  -8.10726643e-01,  -5.69559276e-01,\n",
       "          -1.37073264e-01,  -2.05137148e-01,  -4.20069471e-02],\n",
       "        [ -1.45136309e+00,   8.78992200e-01,   8.56548786e-01,\n",
       "           2.21644759e-01,   6.13842547e-01,  -3.68373394e-01,\n",
       "          -1.07878304e+00,  -1.16971470e-01,  -7.10955501e-01,\n",
       "          -1.57969505e-01,   4.52605546e-01,  -6.66734397e-01,\n",
       "           5.13251126e-01,  -3.20417657e-02,  -5.75110614e-01,\n",
       "           1.62349179e-01,  -5.08150220e-01,   4.98368442e-01,\n",
       "           7.85829127e-01,  -2.13860393e-01,   4.47322577e-01,\n",
       "           6.83291554e-01,  -1.30326793e-01,  -1.69812214e+00,\n",
       "           6.01705253e-01,  -1.06907451e+00,  -1.02502799e+00,\n",
       "           1.82706937e-01,   1.17683363e+00,  -1.63690174e+00],\n",
       "        [ -1.44586694e+00,  -1.42123473e+00,   6.27993584e-01,\n",
       "           1.11970913e+00,   7.05242634e-01,  -8.70657325e-01,\n",
       "           1.15092434e-02,   9.32255566e-01,  -1.13239694e+00,\n",
       "           1.17858537e-01,  -1.21817088e+00,  -1.30907550e-01,\n",
       "          -8.44773769e-01,   2.86439109e+00,  -5.93513966e-01,\n",
       "           7.15471432e-02,   5.54870725e-01,  -7.10677147e-01,\n",
       "          -4.00261790e-01,   9.30176437e-01,   4.25159574e-01,\n",
       "           3.89812261e-01,   5.55084288e-01,  -1.77000254e-01,\n",
       "          -5.85356317e-02,  -5.94147086e-01,  -1.32803321e+00,\n",
       "          -1.85392725e+00,   1.00490880e+00,   1.28933239e+00],\n",
       "        [ -2.50720763e+00,   5.71748734e-01,   1.52379668e+00,\n",
       "           3.33773047e-01,   3.99875253e-01,   1.56631792e+00,\n",
       "           7.03724146e-01,  -2.26371616e-01,   2.75107002e+00,\n",
       "           1.48126945e-01,  -9.49776843e-02,  -5.08530200e-01,\n",
       "           6.39500245e-02,  -3.20734590e-01,   3.26665640e-02,\n",
       "           1.73636425e+00,  -8.73830974e-01,  -1.45899445e-01,\n",
       "          -7.85580873e-01,  -3.33256215e-01,   2.57092166e+00,\n",
       "           5.16505182e-01,   7.77568296e-02,   3.11135918e-01,\n",
       "           3.78878385e-01,   5.58698714e-01,  -3.26885164e-01,\n",
       "          -1.12729871e+00,   7.80579925e-01,   3.24263602e-01],\n",
       "        [ -4.16084647e-01,   6.02786303e-01,  -3.72490376e-01,\n",
       "           7.39885986e-01,  -2.49364421e-01,  -1.12699822e-01,\n",
       "          -5.10104597e-01,  -2.03488320e-01,  -4.24627513e-01,\n",
       "           2.78451383e-01,  -7.84493268e-01,   1.65416110e+00,\n",
       "           2.41935983e-01,   9.23998594e-01,   2.20756817e+00,\n",
       "           6.98963583e-01,   1.75605786e+00,  -9.30724442e-01,\n",
       "          -1.41567481e+00,   5.97375810e-01,   6.94029272e-01,\n",
       "           5.23310661e-01,  -1.02533889e+00,  -8.89284551e-01,\n",
       "          -2.75144488e-01,   1.09073313e-04,   6.63829386e-01,\n",
       "          -8.23759139e-01,  -1.72984183e+00,  -9.64154303e-01],\n",
       "        [  4.40937996e-01,  -1.47142485e-01,   1.88209966e-01,\n",
       "          -8.95607650e-01,  -1.43672943e+00,   8.75043631e-01,\n",
       "           1.60220814e+00,  -9.58266735e-01,  -1.15913403e+00,\n",
       "           2.08905071e-01,  -1.05311525e+00,   3.96685123e-01,\n",
       "           7.31858730e-01,  -2.07065806e-01,   1.68918538e+00,\n",
       "          -1.04553294e+00,   1.01001585e+00,   1.60469010e-01,\n",
       "           1.67927191e-01,  -1.18366694e+00,   5.22504032e-01,\n",
       "          -1.15604711e+00,  -7.85161614e-01,   8.95190090e-02,\n",
       "           6.92975283e-01,   3.18433940e-01,  -1.58167291e+00,\n",
       "          -1.76981783e+00,   1.18372190e+00,   5.92288911e-01],\n",
       "        [  4.40937996e-01,  -1.47142485e-01,   1.88209966e-01,\n",
       "          -8.95607650e-01,  -1.43672943e+00,   8.75043631e-01,\n",
       "           1.60220814e+00,  -9.58266735e-01,  -1.15913403e+00,\n",
       "           2.08905071e-01,  -1.05311525e+00,   3.96685123e-01,\n",
       "           7.31858730e-01,  -2.07065806e-01,   1.68918538e+00,\n",
       "          -1.04553294e+00,   1.01001585e+00,   1.60469010e-01,\n",
       "           1.67927191e-01,  -1.18366694e+00,   5.22504032e-01,\n",
       "          -1.15604711e+00,  -7.85161614e-01,   8.95190090e-02,\n",
       "           6.92975283e-01,   3.18433940e-01,  -1.58167291e+00,\n",
       "          -1.76981783e+00,   1.18372190e+00,   5.92288911e-01]],\n",
       "\n",
       "       [[ -9.84003723e-01,  -7.94791996e-01,  -7.73520350e-01,\n",
       "          -2.28160071e+00,  -5.37502706e-01,  -2.03937411e+00,\n",
       "          -8.84939969e-01,   1.10121703e+00,  -9.65799272e-01,\n",
       "           4.26832408e-01,   2.95939356e-01,   3.42047483e-01,\n",
       "           6.27541602e-01,  -7.93600500e-01,  -1.31308019e-01,\n",
       "           5.49989223e-01,  -3.97993892e-01,  -1.89897883e+00,\n",
       "          -1.99039683e-01,  -3.50875765e-01,  -7.76754737e-01,\n",
       "           1.54270971e+00,   1.83468536e-01,  -1.88466787e-01,\n",
       "          -1.29571176e+00,   1.13859646e-01,   5.35271049e-01,\n",
       "          -1.75219744e-01,   1.29709387e+00,  -1.44457912e+00],\n",
       "        [  1.72790325e+00,   4.63387251e+00,  -3.35472107e-01,\n",
       "           7.70111203e-01,   8.33825052e-01,   7.08714128e-02,\n",
       "           3.80279928e-01,  -1.34867832e-01,   1.91255048e-01,\n",
       "          -8.69557440e-01,  -1.98200122e-01,   8.26152980e-01,\n",
       "          -2.00165305e-02,  -5.69229245e-01,  -1.07406175e+00,\n",
       "          -5.03180802e-01,   1.41938314e-01,   1.26850128e+00,\n",
       "          -8.32317948e-01,  -1.24125469e+00,  -1.43478251e+00,\n",
       "           9.12719667e-02,  -1.13374698e+00,   1.10766661e+00,\n",
       "           8.78830478e-02,  -8.32758784e-01,   5.84480107e-01,\n",
       "           4.22487587e-01,  -3.22953254e-01,  -4.52594876e-01],\n",
       "        [ -1.44586694e+00,  -1.42123473e+00,   6.27993584e-01,\n",
       "           1.11970913e+00,   7.05242634e-01,  -8.70657325e-01,\n",
       "           1.15092434e-02,   9.32255566e-01,  -1.13239694e+00,\n",
       "           1.17858537e-01,  -1.21817088e+00,  -1.30907550e-01,\n",
       "          -8.44773769e-01,   2.86439109e+00,  -5.93513966e-01,\n",
       "           7.15471432e-02,   5.54870725e-01,  -7.10677147e-01,\n",
       "          -4.00261790e-01,   9.30176437e-01,   4.25159574e-01,\n",
       "           3.89812261e-01,   5.55084288e-01,  -1.77000254e-01,\n",
       "          -5.85356317e-02,  -5.94147086e-01,  -1.32803321e+00,\n",
       "          -1.85392725e+00,   1.00490880e+00,   1.28933239e+00],\n",
       "        [ -2.50720763e+00,   5.71748734e-01,   1.52379668e+00,\n",
       "           3.33773047e-01,   3.99875253e-01,   1.56631792e+00,\n",
       "           7.03724146e-01,  -2.26371616e-01,   2.75107002e+00,\n",
       "           1.48126945e-01,  -9.49776843e-02,  -5.08530200e-01,\n",
       "           6.39500245e-02,  -3.20734590e-01,   3.26665640e-02,\n",
       "           1.73636425e+00,  -8.73830974e-01,  -1.45899445e-01,\n",
       "          -7.85580873e-01,  -3.33256215e-01,   2.57092166e+00,\n",
       "           5.16505182e-01,   7.77568296e-02,   3.11135918e-01,\n",
       "           3.78878385e-01,   5.58698714e-01,  -3.26885164e-01,\n",
       "          -1.12729871e+00,   7.80579925e-01,   3.24263602e-01],\n",
       "        [  2.78011560e+00,  -3.21666002e-01,   1.40934932e+00,\n",
       "           2.13962477e-02,  -1.29225878e-02,   6.68657005e-01,\n",
       "          -6.27840400e-01,  -3.64384502e-01,  -9.66149390e-01,\n",
       "          -3.71809751e-01,  -7.68156290e-01,  -6.44540966e-01,\n",
       "          -5.33378944e-02,   2.36983821e-01,   1.34070918e-01,\n",
       "           1.52141094e-01,   1.55224574e+00,   1.40769586e-01,\n",
       "          -3.29185814e-01,  -8.73762965e-01,  -1.22099984e+00,\n",
       "           8.18111002e-01,  -1.20147574e+00,   1.21608615e+00,\n",
       "          -4.34667945e-01,  -9.48471010e-01,   3.72291684e-01,\n",
       "          -7.89611280e-01,  -6.71473861e-01,   1.65397182e-01],\n",
       "        [  4.40937996e-01,  -1.47142485e-01,   1.88209966e-01,\n",
       "          -8.95607650e-01,  -1.43672943e+00,   8.75043631e-01,\n",
       "           1.60220814e+00,  -9.58266735e-01,  -1.15913403e+00,\n",
       "           2.08905071e-01,  -1.05311525e+00,   3.96685123e-01,\n",
       "           7.31858730e-01,  -2.07065806e-01,   1.68918538e+00,\n",
       "          -1.04553294e+00,   1.01001585e+00,   1.60469010e-01,\n",
       "           1.67927191e-01,  -1.18366694e+00,   5.22504032e-01,\n",
       "          -1.15604711e+00,  -7.85161614e-01,   8.95190090e-02,\n",
       "           6.92975283e-01,   3.18433940e-01,  -1.58167291e+00,\n",
       "          -1.76981783e+00,   1.18372190e+00,   5.92288911e-01],\n",
       "        [  4.40937996e-01,  -1.47142485e-01,   1.88209966e-01,\n",
       "          -8.95607650e-01,  -1.43672943e+00,   8.75043631e-01,\n",
       "           1.60220814e+00,  -9.58266735e-01,  -1.15913403e+00,\n",
       "           2.08905071e-01,  -1.05311525e+00,   3.96685123e-01,\n",
       "           7.31858730e-01,  -2.07065806e-01,   1.68918538e+00,\n",
       "          -1.04553294e+00,   1.01001585e+00,   1.60469010e-01,\n",
       "           1.67927191e-01,  -1.18366694e+00,   5.22504032e-01,\n",
       "          -1.15604711e+00,  -7.85161614e-01,   8.95190090e-02,\n",
       "           6.92975283e-01,   3.18433940e-01,  -1.58167291e+00,\n",
       "          -1.76981783e+00,   1.18372190e+00,   5.92288911e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_A.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Variable' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-da64ed599925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmem_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Variable' object is not callable"
     ]
    }
   ],
   "source": [
    "mem_network.encoding(embed_A.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1000, 200, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_stories), len(train_questions), len(test_stories), len(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_tensor(data):\n",
    "    return Variable(torch.LongTensor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def making_data_set(stories, questions, mode='single_task'):\n",
    "    total_set_number = len(questions.keys())\n",
    "#     data = []\n",
    "    for k in range(total_set_number):\n",
    "        q = questions[k]\n",
    "        story = stories[q['story_index']][q['sentence_index']-1 : q['sentence_index']+1]\n",
    "        query = q['question']\n",
    "        answer = q['answer']\n",
    "        yield (data_tensor(story), data_tensor(query), data_tensor(answer))\n",
    "#         data.append((data_tensor(story), data_tensor(query), data_tensor(answer)))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mem_network.parameters(), lr=0.01)\n",
    "loss_F = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_data,\n",
    "                           batch_size=config['batch_size'],\n",
    "                           num_workers=1,\n",
    "                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_single_epoch(train_stories, train_questions):\n",
    "    for (story, query, answer) in making_data_set(train_stories, train_questions):\n",
    "        optimizer.zero_grad()\n",
    "        a_hat, probs = mem_network.forward(story, query)\n",
    "        \n",
    "        loss = loss_F(a_hat, answer)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(stories, questions, model):\n",
    "    acc = 0\n",
    "    len_data_set = len(questions)\n",
    "    for (story, query, answer) in making_data_set(stories, questions):\n",
    "        a_hat, pred_probs = model.forward(story, query)\n",
    "        pred = pred_probs.data.max(1)[1]\n",
    "        acc += pred.eq(answer.data).sum()\n",
    "    \n",
    "    return acc / len_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#10: loss: 15.244619 | train_acc: 0.164 | test_acc: 0.162\n",
      "#20: loss: 24.808598 | train_acc: 0.155 | test_acc: 0.171\n",
      "#30: loss: 3.690599 | train_acc: 0.151 | test_acc: 0.165\n",
      "#40: loss: 6.681912 | train_acc: 0.173 | test_acc: 0.169\n",
      "#50: loss: 2.834771 | train_acc: 0.167 | test_acc: 0.172\n",
      "#60: loss: 15.150473 | train_acc: 0.166 | test_acc: 0.182\n",
      "#70: loss: 9.215902 | train_acc: 0.163 | test_acc: 0.177\n",
      "#80: loss: 1.771870 | train_acc: 0.177 | test_acc: 0.175\n",
      "#90: loss: 26.715284 | train_acc: 0.163 | test_acc: 0.176\n",
      "#100: loss: 22.503906 | train_acc: 0.166 | test_acc: 0.182\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['nepoch']):\n",
    "    loss = train_single_epoch(train_stories, train_questions)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        train_acc = evaluate(train_stories, train_questions, mem_network)\n",
    "        test_acc = evaluate(test_stories, test_questions, mem_network)\n",
    "        print('#{0}: loss: {1:.6f} | train_acc: {2} | test_acc: {3}'.format(\n",
    "                                                            epoch+1, loss, train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
