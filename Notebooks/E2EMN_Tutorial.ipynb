{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from memn2n.bAbIDataSet_utils import bAbI\n",
    "from memn2n.utils import get_story_idx, matshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 1\n",
    "BATCH = 2\n",
    "FIX_STORY = None\n",
    "DEVICE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "babi = bAbI()\n",
    "train, valid, test = babi.splits(root='../data/QA_bAbI_tasks/en-valid-10k/', task=TASK, fix_maxlen_story=FIX_STORY,\n",
    "                                 device=DEVICE)\n",
    "train_loader, valid_loader, test_loader = babi.iters(train, valid, test, BATCH)\n",
    "VOCAB = train.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 6]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([2, 1]),\n",
       " torch.Size([2, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories, queries, answers = b\n",
    "stories_idx = get_story_idx(stories, 0)\n",
    "print(stories_idx)\n",
    "stories.size(), queries.size(), answers.size(), stories_idx.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED = 10\n",
    "N_HOPS = 3\n",
    "MAXLEN = train.maxlen_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adjacent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 6, 10]), torch.Size([2, 3, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_a = embedding(stories)  # B, len_story, len_words, embed_size\n",
    "embeded_b = embedding(queries)  # B, len_query, embed_size\n",
    "embeded_a.size(), embeded_b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. RNN Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_A = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "embedding_B = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "embedding_C = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 6, 10]), torch.Size([2, 3, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_a = embedding_A(stories)  # B, len_story, len_words, embed_size\n",
    "embeded_b = embedding_B(queries)  # B, len_query, embed_size\n",
    "embeded_a.size(), embeded_b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_i = \\sum_j l_j \\odot A x_{ij}$\n",
    "\n",
    "$l_{kj} = (1-\\frac{j}{J}) - (\\frac{k}{d})(1-\\frac{2j}{J})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position encoding: B, len_story,, len_words(J), embed_size(d) \n",
    "B, len_story, len_words, embed_size = embeded_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.ones_like(embeded_a, device='cpu')\n",
    "k = temp * torch.arange(1, embed_size+1, dtype=torch.float) / embed_size\n",
    "l = temp * torch.arange(1, len_words+1, dtype=torch.float).unsqueeze(1) / len_words\n",
    "position = (1- l) - k * (1 - 2*l)\n",
    "position_encoded = (embeded_a * position.to(embeded_a.device)).sum(2)\n",
    "position_encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one of position matrix like\n",
    "matshow(position[0, 0, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_i = \\sum_j Ax_{ij} + T_A(i)$\n",
    "\n",
    "Note that sentences are indexed in reverse order, reflecting their relative distance from the question so that $x_1$ is the last sentence of the story.\n",
    "\n",
    "temporal context encoding length will be \"max story + 1\" because of pad stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 3, 2, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_A = nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "temporal_C = nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "temporal_A(stories_idx).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is no \"temporal encoding\" and \"position encoding\"\n",
    "\n",
    "$\\begin{aligned} \n",
    "m_i &= \\sum_j Ax_{ij} \\\\\n",
    "u &= \\sum_j Bq_{j} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 10]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = embeded_a.sum(2)  # B, len_story, len_words, embed_size > B, len_story, embed_size\n",
    "u = embeded_b.sum(1)  # B, len_query, embed_size > B, embed_size\n",
    "m.size(), u.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_i = \\text{softmax}(m_i \\cdot u)$\n",
    "\n",
    "If Linear Start scores don't go through softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 10]), torch.Size([2, 10, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS = False\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "if LS:\n",
    "    p = torch.bmm(m, u.unsqueeze(2))  # B, len_story, 1\n",
    "else:\n",
    "    p = softmax_layer(torch.bmm(m, u.unsqueeze(2)))\n",
    "embeded_c = embedding_C(stories)  # B, len_story, len_sent, embed_size\n",
    "# embeded_c = embeddding(stories) : for adjacent weight style\n",
    "c = embeded_c.sum(2)  # B, len_story, len_sent, embed_size > B, len_story, embed_size\n",
    "c.size(), p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.994 0.    0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001]\n",
      " [0.185 0.    0.026 0.789 0.    0.    0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(p.detach().squeeze(2).numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matshow(p.detach().squeeze(2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o = \\displaystyle\\sum_i p_i c_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = (c * p).sum(1)  # B, len_story, embed_size > B, embed_size\n",
    "o.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{a} = \\text{softmax}(W(H \\cdot u + o))$\n",
    "\n",
    "linear mapping for \"rnnlike\" weight style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mapping = nn.Linear(embed_size, embed_size)  # helpful linear mapping for each hops\n",
    "linear_final = nn.Linear(embed_size, len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_next = linear_mapping(u) + o\n",
    "u_next.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = linear_final(u_next)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers for multi hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adjacent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                  nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                for k, n in enumerate(['A', 'C']*N_HOPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 10])\n"
     ]
    }
   ],
   "source": [
    "for name, mod in context_modules.items():\n",
    "    idx = int(name.split('_')[-1])\n",
    "    if idx == 0:\n",
    "        embedding_B = mod\n",
    "        print(mod.weight.size())\n",
    "    elif idx % 2 == 0:\n",
    "        mod.weight.data = context_modules['embedding_C_{}'.format(idx-1)].weight.data\n",
    "    elif idx == (N_HOPS*2-1):\n",
    "        linear_final = nn.Linear(EMBED, len(VOCAB))\n",
    "        linear_final.weight.data = mod.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_modules['embedding_C_1'].weight.sum() == context_modules['embedding_A_2'].weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. RNN Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                  nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                for k, n in enumerate(['A', 'C']*N_HOPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "for name, mod in context_modules.items():\n",
    "    idx = int(name.split('_')[-1])\n",
    "    if idx <= 1 :\n",
    "        continue\n",
    "    elif idx % 2 == 0:\n",
    "        mod.weight.data = context_modules['embedding_A_0'].weight.data\n",
    "    else:\n",
    "        mod.weight.data = context_modules['embedding_C_1'].weight.data\n",
    "\n",
    "# others layers\n",
    "embedding_B = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "linear_mapping = nn.Linear(embed_size, embed_size)  # helpful linear mapping for each hops\n",
    "linear_final = nn.Linear(embed_size, len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_modules['embedding_A_0'].weight.sum() == context_modules['embedding_A_2'].weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Common: temporal encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (temporal_A): Embedding(11, 10, padding_idx=0)\n",
       "  (temporal_C): Embedding(11, 10, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_layer = nn.Softmax(dim=1)\n",
    "temporal_modules = nn.ModuleDict([('temporal_{}'.format(n), \n",
    "                                   nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                  for n in ['A', 'C']])\n",
    "temporal_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMN2N(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, weight_style='adjacent',\n",
    "                 encoding_method='bow', temporal=True, maxlen_story=None, pad_idx=0):\n",
    "        \"\"\"\n",
    "        https://arxiv.org/pdf/1503.08895.pdf\n",
    "        Args:\n",
    "        - vocab_size: length of vocabulary\n",
    "        - embed_size: size of embedding dimension  \n",
    "        - n_hops: multiple computational steps for memories\n",
    "        - weight_style: \n",
    "            * 'adjacent': share all weights B = A(1) = C(1) = ... = C(K) = W^T\n",
    "            * 'rnnlike': share weights \n",
    "        - encoding_method:\n",
    "        - temporal:\n",
    "        - maxlen_story:\n",
    "        - pad_idx:\n",
    "        \"\"\"\n",
    "        super(MEMN2N, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.maxlen_story = maxlen_story\n",
    "        self.weight_style = weight_style.lower()\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.layers_init()\n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "    def layers_init(self):\n",
    "        \"\"\"\n",
    "        two types: adjacent, rnnlike\n",
    "        \"\"\"\n",
    "        self.context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                               nn.Embedding(self.vocab_size, \n",
    "                                                            self.embed_size,\n",
    "                                                            padding_idx=self.pad_idx)) \\\n",
    "                                              for k, n in enumerate(['A', 'C']*self.n_hops)])\n",
    "        # adjacent weight sharing style\n",
    "        if self.weight_style == 'adjacent':\n",
    "            for name, mod in self.context_modules.items():\n",
    "                idx = int(name.split('_')[-1])\n",
    "                if idx == 0:\n",
    "                    self.embedding_B = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=self.pad_idx)\n",
    "                    self.embedding_B.weight.data = mod.weight.data \n",
    "                elif idx % 2 == 0:\n",
    "                    mod.weight.data = self.context_modules['embedding_C_{}'.format(idx-1)].weight.data\n",
    "                elif idx == (self.n_hops*2-1):\n",
    "                    self.linear_final = nn.Linear(self.embed_size, self.vocab_size, bias=False)\n",
    "                    self.linear_final.weight.data = mod.weight.data\n",
    "            \n",
    "        # rnn-like weight sharing style        \n",
    "        elif self.weight_style == 'rnnlike':\n",
    "            for name, mod in self.context_modules.items():\n",
    "                idx = int(name.split('_')[-1])\n",
    "                if idx <= 1 :\n",
    "                    continue\n",
    "                elif idx % 2 == 0:\n",
    "                    mod.weight.data = self.context_modules['embedding_A_0'].weight.data\n",
    "                else:\n",
    "                    mod.weight.data = self.context_modules['embedding_C_1'].weight.data\n",
    "\n",
    "            # others layers\n",
    "            self.embedding_B = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=self.pad_idx)\n",
    "            self.linear_mapping = nn.Linear(self.embed_size, self.embed_size)  # inear mapping for each hops\n",
    "            self.linear_final = nn.Linear(self.embed_size, self.vocab_size, bias=False)\n",
    "        else:\n",
    "            assert True, 'Insert \"adjacent\" or \"rnnlike\" in weight_style'\n",
    "        # common\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "        if self.te:\n",
    "            self.temporal_modules = nn.ModuleDict([('temporal_{}'.format(n), \n",
    "                                                    nn.Embedding(self.maxlen_story+1, \n",
    "                                                                 self.embed_size, \n",
    "                                                                 padding_idx=self.pad_idx)) \\\n",
    "                                                   for n in ['A', 'C']])\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight.data, mean=0, std=0.1)\n",
    "            m.weight.data[0] = 0\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight.data, mean=0, std=0.1)\n",
    "            if self.weight_style == 'adjacent':\n",
    "                m.weight.data[0] = 0\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "                \n",
    "    def encoding2memory(self, embeded_x):\n",
    "        sum_dim = 2 if embeded_x.dim() == 4 else 1  # stories sum_dim=2, queries sum_dim=1\n",
    "        *_, len_words, embed_size = embeded_x.size()  # len_words=J, embed_size=d\n",
    "        \n",
    "        if self.encoding_method == 'bow':\n",
    "            return embeded_x.sum(sum_dim)\n",
    "        \n",
    "        if self.encoding_method == 'pe':\n",
    "            temp = torch.ones_like(embeded_x, device='cpu')\n",
    "            k = temp * torch.arange(1, embed_size+1, dtype=torch.float) / embed_size\n",
    "            l = temp * torch.arange(1, len_words+1, dtype=torch.float).unsqueeze(1) / len_words\n",
    "            position = (1- l) - k * (1 - 2*l)\n",
    "            position_encoded = (embeded_x * position.to(embeded_x.device)).sum(sum_dim)\n",
    "            return position_encoded  # B, (len_story), embed_size\n",
    "        \n",
    "        else:\n",
    "            assert True, 'Insert \"bow\" or \"pe\" in encoding_method'\n",
    "            \n",
    "    def forward(self, stories, queries, stories_idx, ls=False, return_p=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - stories: B, maxlen_story(T), maxlen_words(n)\n",
    "        - queries: B, maxlen_query(T_q)\n",
    "        - stories_idx: B, maxlen_story(T)\n",
    "        - ls: linear start\n",
    "        Outputs:\n",
    "        - log softmaxed score(nll loss)\n",
    "        \"\"\"            \n",
    "        # Start Learning\n",
    "        embeded_b = self.embedding_B(queries) \n",
    "        u_next = self.encoding2memory(embeded_b)  # (B, d)\n",
    "        ps = []\n",
    "        for k in range(self.n_hops):\n",
    "            embeded_a = self.context_modules['embedding_A_{}'.format(2*k)](stories)  # (B, T, n, d)\n",
    "            embeded_c = self.context_modules['embedding_C_{}'.format(2*k+1)](stories)  # (B, T, n, d)\n",
    "            m = self.encoding2memory(embeded_a)  # (B, T, d)\n",
    "            c = self.encoding2memory(embeded_c)  # (B, T, d)\n",
    "            if self.te:\n",
    "                m += self.temporal_modules['temporal_A'](stories_idx)  # (B, T, d)\n",
    "                c += self.temporal_modules['temporal_C'](stories_idx)  # (B, T, d)\n",
    "            if ls:\n",
    "                # (B, T, d) x (B, d, 1) = (B, T, 1)\n",
    "                p = torch.bmm(m, u_next.unsqueeze(2))\n",
    "            else:\n",
    "                p = self.softmax_layer(torch.bmm(m, u_next.unsqueeze(2)))\n",
    "                ps.append(p.squeeze(2))  # [(B, T) for all hops]\n",
    "            o = (c * p).sum(1)  # (B, T, d) * (B, T, 1) = (B, T, d) > (B, d)\n",
    "            \n",
    "            if self.weight_style == 'rnnlike':\n",
    "                # (B, d) + (B, d) = (B, d)\n",
    "                u_next = self.linear_mapping(u_next) + o\n",
    "            else:\n",
    "                u_next = u_next + o\n",
    "                \n",
    "        a = self.linear_final(u_next)  # (B, d) > (B, V)\n",
    "        if return_p:\n",
    "            return a, p\n",
    "        return torch.log_softmax(a, dim=1)  # use nll loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memn2n = MEMN2N(vocab_size=len(VOCAB), embed_size=15, weight_style='adjacent', \n",
    "                encoding_method='bow', temporal=True, maxlen_story=train.maxlen_story,\n",
    "                pad_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "memn2n.train()\n",
    "memn2n.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters(model, module_name):\n",
    "    print('[layer-{}] sum of papmerters: {:.4f}'.format(module_name, \n",
    "                                                    model.context_modules[module_name].weight.sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[layer-embedding_C_1] sum of papmerters: 3.1963\n",
      "[layer-embedding_A_2] sum of papmerters: 3.1963\n"
     ]
    }
   ],
   "source": [
    "check_parameters(memn2n, 'embedding_C_1')\n",
    "check_parameters(memn2n, 'embedding_A_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = memn2n(stories, queries, stories_idx, ls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/valueerror-cant-optimize-a-non-leaf-tensor/21751/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = optim.SGD(memn2n.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.1856, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(nll_loss, answers.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[layer-embedding_C_1] sum of papmerters: 3.1944\n",
      "[layer-embedding_A_2] sum of papmerters: 3.1944\n"
     ]
    }
   ],
   "source": [
    "check_parameters(memn2n, 'embedding_C_1')\n",
    "check_parameters(memn2n, 'embedding_A_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "TASK = 1\n",
    "BATCH = 32\n",
    "FIX_STORY = None\n",
    "\n",
    "EMBED = 30\n",
    "W_STYLE = 'adjacent'\n",
    "ENC_METHOD = 'bow'\n",
    "TEMPROAL = False\n",
    "STEP = 10\n",
    "LR = 0.01\n",
    "ANNEAL = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "babi = bAbI()\n",
    "train, valid, test = babi.splits(root='../data/QA_bAbI_tasks/en-valid-10k/', \n",
    "                                 task=TASK, \n",
    "                                 fix_maxlen_story=FIX_STORY,\n",
    "                                 device=DEVICE)\n",
    "train_loader, valid_loader, test_loader = babi.iters(train, valid, test, BATCH)\n",
    "VOCAB = train.vocab\n",
    "MAXLEN = train.maxlen_story\n",
    "\n",
    "memn2n = MEMN2N(vocab_size=len(VOCAB), embed_size=15, weight_style='rnnlike', \n",
    "                encoding_method='pe', temporal=True, maxlen_story=MAXLEN,\n",
    "                pad_idx=VOCAB.stoi['<pad>']).to(DEVICE)\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=VOCAB.stoi['<pad>'], reduction='sum')\n",
    "optimizer = optim.SGD(memn2n.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=ANNEAL, milestones=[25, 50, 75], \n",
    "                                           optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] loss: 60.7465, lr: 0.01,\n",
      "[2/10] loss: 56.6869, lr: 0.01,\n",
      "[3/10] loss: 44.6670, lr: 0.01,\n",
      "[4/10] loss: 7.4042, lr: 0.01,\n",
      "[5/10] loss: 0.0937, lr: 0.01,\n",
      "[6/10] loss: 0.0498, lr: 0.01,\n",
      "[7/10] loss: 0.0351, lr: 0.01,\n",
      "[8/10] loss: 0.0273, lr: 0.01,\n",
      "[9/10] loss: 0.0223, lr: 0.01,\n",
      "[10/10] loss: 0.0189, lr: 0.01,\n"
     ]
    }
   ],
   "source": [
    "for step in range(STEP):\n",
    "    \n",
    "    scheduler.step()\n",
    "    for b in train_loader:\n",
    "        stories, queries, answers = b\n",
    "        stories_idx = get_story_idx(stories, VOCAB.stoi['<pad>'])\n",
    "        memn2n.zero_grad()\n",
    "        \n",
    "        nll_loss = memn2n(stories, queries, stories_idx, ls=False)\n",
    "        loss = loss_function(nll_loss, answers.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(memn2n.parameters(), 50.0)  # gradient clipping\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    if step % 1 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {},'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
