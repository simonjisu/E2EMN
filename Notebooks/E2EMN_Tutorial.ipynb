{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1] + ['memn2n']))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from bAbIDataSet_utils import bAbI\n",
    "from utils import get_story_idx, matshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 19\n",
    "BATCH = 2\n",
    "FIX_STORY = None\n",
    "DEVICE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "babi = bAbI()\n",
    "train, valid, test = babi.splits(root='../data/QA_bAbI_tasks/en-valid-10k/', task=TASK, fix_maxlen_story=FIX_STORY,\n",
    "                                 device=DEVICE)\n",
    "train_loader, valid_loader, test_loader = babi.iters(train, valid, test, BATCH)\n",
    "VOCAB = train.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 4, 3, 2, 1],\n",
      "        [5, 4, 3, 2, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 7]),\n",
       " torch.Size([2, 10]),\n",
       " torch.Size([2, 1]),\n",
       " torch.Size([2, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories, queries, answers = b\n",
    "stories_idx = get_story_idx(stories, 0)\n",
    "print(stories_idx)\n",
    "stories.size(), queries.size(), answers.size(), stories_idx.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED = 10\n",
    "N_HOPS = 3\n",
    "MAXLEN = train.maxlen_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adjacent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 6, 10]), torch.Size([2, 3, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_a = embedding(stories)  # B, len_story, len_words, embed_size\n",
    "embeded_b = embedding(queries)  # B, len_query, embed_size\n",
    "embeded_a.size(), embeded_b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. RNN Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_A = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "embedding_B = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "embedding_C = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 7, 10]), torch.Size([2, 10, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_a = embedding_A(stories)  # B, len_story, len_words, embed_size\n",
    "embeded_b = embedding_B(queries)  # B, len_query, embed_size\n",
    "embeded_a.size(), embeded_b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_i = \\sum_j l_j \\odot A x_{ij}$\n",
    "\n",
    "$l_{kj} = (1-\\frac{j}{J}) - (\\frac{k}{d})(1-\\frac{2j}{J})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position encoding: B, len_story,, len_words(J), embed_size(d) \n",
    "B, len_story, len_words, embed_size = embeded_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.ones_like(embeded_a, device='cpu')\n",
    "k = temp * torch.arange(1, embed_size+1, dtype=torch.float) / embed_size\n",
    "l = temp * torch.arange(1, len_words+1, dtype=torch.float).unsqueeze(1) / len_words\n",
    "position = (1- l) - k * (1 - 2*l)\n",
    "position_encoded = (embeded_a * position.to(embeded_a.device)).sum(2)\n",
    "position_encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one of position matrix like\n",
    "matshow(position[0, 0, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_i = \\sum_j Ax_{ij} + T_A(i)$\n",
    "\n",
    "Note that sentences are indexed in reverse order, reflecting their relative distance from the question so that $x_1$ is the last sentence of the story.\n",
    "\n",
    "temporal context encoding length will be \"max story + 1\" because of pad stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 4, 3, 2, 1],\n",
       "        [5, 4, 3, 2, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_A = nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "temporal_C = nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "temporal_A(stories_idx).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is no \"temporal encoding\" and \"position encoding\"\n",
    "\n",
    "$\\begin{aligned} \n",
    "m_i &= \\sum_j Ax_{ij} \\\\\n",
    "u &= \\sum_j Bq_{j} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = embeded_a.sum(2)  # B, len_story, len_words, embed_size > B, len_story, embed_size\n",
    "u = embeded_b.sum(1)  # B, len_query, embed_size > B, embed_size\n",
    "m.size(), u.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_i = \\text{softmax}(m_i \\cdot u)$\n",
    "\n",
    "If Linear Start scores don't go through softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 5, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS = False\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "if LS:\n",
    "    p = torch.bmm(m, u.unsqueeze(2))  # B, len_story, 1\n",
    "else:\n",
    "    p = softmax_layer(torch.bmm(m, u.unsqueeze(2)))\n",
    "embeded_c = embedding_C(stories)  # B, len_story, len_sent, embed_size\n",
    "# embeded_c = embeddding(stories) : for adjacent weight style\n",
    "c = embeded_c.sum(2)  # B, len_story, len_sent, embed_size > B, len_story, embed_size\n",
    "c.size(), p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(p.detach().squeeze(2).numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matshow(p.detach().squeeze(2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o = \\displaystyle\\sum_i p_i c_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = (c * p).sum(1)  # B, len_story, embed_size > B, embed_size\n",
    "o.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{a} = \\text{softmax}(W(H \\cdot u + o))$\n",
    "\n",
    "linear mapping for \"rnnlike\" weight style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_mapping = nn.Linear(embed_size, embed_size)  # helpful linear mapping for each hops\n",
    "linear_final = nn.Linear(embed_size, len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_next = linear_mapping(u) + o\n",
    "u_next.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = linear_final(u_next)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers for multi hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adjacent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                  nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                for k, n in enumerate(['A', 'C']*N_HOPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 10])\n"
     ]
    }
   ],
   "source": [
    "for name, mod in context_modules.items():\n",
    "    idx = int(name.split('_')[-1])\n",
    "    if idx == 0:\n",
    "        embedding_B = mod\n",
    "        print(mod.weight.size())\n",
    "    elif idx % 2 == 0:\n",
    "        mod.weight.data = context_modules['embedding_C_{}'.format(idx-1)].weight.data\n",
    "    elif idx == (N_HOPS*2-1):\n",
    "        linear_final = nn.Linear(EMBED, len(VOCAB))\n",
    "        linear_final.weight.data = mod.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_modules['embedding_C_1'].weight.sum() == context_modules['embedding_A_2'].weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. RNN Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                  nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                for k, n in enumerate(['A', 'C']*N_HOPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleList will be shorter code, but for convience looking i used ModuleDict\n",
    "for name, mod in context_modules.items():\n",
    "    idx = int(name.split('_')[-1])\n",
    "    if idx <= 1 :\n",
    "        continue\n",
    "    elif idx % 2 == 0:\n",
    "        mod.weight.data = context_modules['embedding_A_0'].weight.data\n",
    "    else:\n",
    "        mod.weight.data = context_modules['embedding_C_1'].weight.data\n",
    "\n",
    "# others layers\n",
    "embedding_B = nn.Embedding(len(VOCAB), EMBED, padding_idx=VOCAB.stoi['<pad>'])\n",
    "linear_mapping = nn.Linear(embed_size, embed_size)  # helpful linear mapping for each hops\n",
    "linear_final = nn.Linear(embed_size, len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_modules['embedding_A_0'].weight.sum() == context_modules['embedding_A_2'].weight.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Common: temporal encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (temporal_A): Embedding(11, 10, padding_idx=0)\n",
       "  (temporal_C): Embedding(11, 10, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_layer = nn.Softmax(dim=1)\n",
    "temporal_modules = nn.ModuleDict([('temporal_{}'.format(n), \n",
    "                                   nn.Embedding(MAXLEN+1, EMBED, padding_idx=VOCAB.stoi['<pad>'])) \\\n",
    "                                  for n in ['A', 'C']])\n",
    "temporal_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMN2N(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_hops=3, weight_style='adjacent',\n",
    "                 encoding_method='bow', temporal=True, maxlen_story=None, pad_idx=0):\n",
    "        \"\"\"\n",
    "        https://arxiv.org/pdf/1503.08895.pdf\n",
    "        Args:\n",
    "        - vocab_size: length of vocabulary\n",
    "        - embed_size: size of embedding dimension  \n",
    "        - n_hops: multiple computational steps for memories\n",
    "        - weight_style: \n",
    "            * 'adjacent': share all weights B = A(1) = C(1) = ... = C(K) = W^T\n",
    "            * 'rnnlike': share weights \n",
    "        - encoding_method:\n",
    "        - temporal:\n",
    "        - maxlen_story:\n",
    "        - pad_idx:\n",
    "        \"\"\"\n",
    "        super(MEMN2N, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.n_hops = n_hops\n",
    "        self.maxlen_story = maxlen_story\n",
    "        self.weight_style = weight_style.lower()\n",
    "        self.encoding_method = encoding_method.lower()\n",
    "        self.te = temporal\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.layers_init()\n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "    def layers_init(self):\n",
    "        \"\"\"\n",
    "        two types: adjacent, rnnlike\n",
    "        \"\"\"\n",
    "        self.context_modules = nn.ModuleDict([('embedding_{}_{}'.format(n, k), \n",
    "                                               nn.Embedding(self.vocab_size, \n",
    "                                                            self.embed_size,\n",
    "                                                            padding_idx=self.pad_idx)) \\\n",
    "                                              for k, n in enumerate(['A', 'C']*self.n_hops)])\n",
    "        # adjacent weight sharing style\n",
    "        if self.weight_style == 'adjacent':\n",
    "            for name, mod in self.context_modules.items():\n",
    "                idx = int(name.split('_')[-1])\n",
    "                if idx == 0:\n",
    "                    self.embedding_B = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=self.pad_idx)\n",
    "                    self.embedding_B.weight.data = mod.weight.data \n",
    "                elif idx % 2 == 0:\n",
    "                    mod.weight.data = self.context_modules['embedding_C_{}'.format(idx-1)].weight.data\n",
    "                elif idx == (self.n_hops*2-1):\n",
    "                    self.linear_final = nn.Linear(self.embed_size, self.vocab_size, bias=False)\n",
    "                    self.linear_final.weight.data = mod.weight.data\n",
    "            \n",
    "        # rnn-like weight sharing style        \n",
    "        elif self.weight_style == 'rnnlike':\n",
    "            for name, mod in self.context_modules.items():\n",
    "                idx = int(name.split('_')[-1])\n",
    "                if idx <= 1 :\n",
    "                    continue\n",
    "                elif idx % 2 == 0:\n",
    "                    mod.weight.data = self.context_modules['embedding_A_0'].weight.data\n",
    "                else:\n",
    "                    mod.weight.data = self.context_modules['embedding_C_1'].weight.data\n",
    "\n",
    "            # others layers\n",
    "            self.embedding_B = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=self.pad_idx)\n",
    "            self.linear_mapping = nn.Linear(self.embed_size, self.embed_size)  # inear mapping for each hops\n",
    "            self.linear_final = nn.Linear(self.embed_size, self.vocab_size, bias=False)\n",
    "        else:\n",
    "            assert True, 'Insert \"adjacent\" or \"rnnlike\" in weight_style'\n",
    "        # common\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    "        if self.te:\n",
    "            self.temporal_modules = nn.ModuleDict([('temporal_{}'.format(n), \n",
    "                                                    nn.Embedding(self.maxlen_story+1, \n",
    "                                                                 self.embed_size, \n",
    "                                                                 padding_idx=self.pad_idx)) \\\n",
    "                                                   for n in ['A', 'C']])\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight.data, mean=0, std=0.1)\n",
    "            m.weight.data[0] = 0\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight.data, mean=0, std=0.1)\n",
    "            if self.weight_style == 'adjacent':\n",
    "                m.weight.data[0] = 0\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "                \n",
    "    def encoding2memory(self, embeded_x):\n",
    "        sum_dim = 2 if embeded_x.dim() == 4 else 1  # stories sum_dim=2, queries sum_dim=1\n",
    "        *_, len_words, embed_size = embeded_x.size()  # len_words=J, embed_size=d\n",
    "        \n",
    "        if self.encoding_method == 'bow':\n",
    "            return embeded_x.sum(sum_dim)\n",
    "        \n",
    "        if self.encoding_method == 'pe':\n",
    "            temp = torch.ones_like(embeded_x, device='cpu')\n",
    "            k = temp * torch.arange(1, embed_size+1, dtype=torch.float) / embed_size\n",
    "            l = temp * torch.arange(1, len_words+1, dtype=torch.float).unsqueeze(1) / len_words\n",
    "            position = (1- l) - k * (1 - 2*l)\n",
    "            position_encoded = (embeded_x * position.to(embeded_x.device)).sum(sum_dim)\n",
    "            return position_encoded  # B, (len_story), embed_size\n",
    "        \n",
    "        else:\n",
    "            assert True, 'Insert \"bow\" or \"pe\" in encoding_method'\n",
    "            \n",
    "    def forward(self, stories, queries, stories_idx, ls=False, return_p=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - stories: B, maxlen_story(T), maxlen_words(n)\n",
    "        - queries: B, maxlen_query(T_q)\n",
    "        - stories_idx: B, maxlen_story(T)\n",
    "        - ls: linear start\n",
    "        Outputs:\n",
    "        - log softmaxed score(nll loss)\n",
    "        \"\"\"            \n",
    "        # Start Learning\n",
    "        embeded_b = self.embedding_B(queries) \n",
    "        u_next = self.encoding2memory(embeded_b)  # (B, d)\n",
    "        ps = []\n",
    "        for k in range(self.n_hops):\n",
    "            embeded_a = self.context_modules['embedding_A_{}'.format(2*k)](stories)  # (B, T, n, d)\n",
    "            embeded_c = self.context_modules['embedding_C_{}'.format(2*k+1)](stories)  # (B, T, n, d)\n",
    "            m = self.encoding2memory(embeded_a)  # (B, T, d)\n",
    "            c = self.encoding2memory(embeded_c)  # (B, T, d)\n",
    "            if self.te:\n",
    "                m += self.temporal_modules['temporal_A'](stories_idx)  # (B, T, d)\n",
    "                c += self.temporal_modules['temporal_C'](stories_idx)  # (B, T, d)\n",
    "            if ls:\n",
    "                # (B, T, d) x (B, d, 1) = (B, T, 1)\n",
    "                p = torch.bmm(m, u_next.unsqueeze(2))\n",
    "            else:\n",
    "                p = self.softmax_layer(torch.bmm(m, u_next.unsqueeze(2)))\n",
    "                ps.append(p.squeeze(2))  # [(B, T) for all hops]\n",
    "            o = (c * p).sum(1)  # (B, T, d) * (B, T, 1) = (B, T, d) > (B, d)\n",
    "            \n",
    "            if self.weight_style == 'rnnlike':\n",
    "                # (B, d) + (B, d) = (B, d)\n",
    "                u_next = self.linear_mapping(u_next) + o\n",
    "            else:\n",
    "                u_next = u_next + o\n",
    "                \n",
    "        a = self.linear_final(u_next)  # (B, d) > (B, V)\n",
    "        if return_p:\n",
    "            return a, ps\n",
    "        return torch.log_softmax(a, dim=1)  # use nll loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memn2n = MEMN2N(vocab_size=len(VOCAB), embed_size=15, weight_style='adjacent', \n",
    "                encoding_method='bow', temporal=True, maxlen_story=train.maxlen_story,\n",
    "                pad_idx=VOCAB.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memn2n.train()\n",
    "memn2n.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters(model, module_name):\n",
    "    print('[layer-{}] sum of papmerters: {:.4f}'.format(module_name, \n",
    "                                                    model.context_modules[module_name].weight.sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[layer-embedding_C_1] sum of papmerters: 2.2103\n",
      "[layer-embedding_A_2] sum of papmerters: 2.2103\n"
     ]
    }
   ],
   "source": [
    "check_parameters(memn2n, 'embedding_C_1')\n",
    "check_parameters(memn2n, 'embedding_A_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = memn2n(stories, queries, stories_idx, ls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/valueerror-cant-optimize-a-non-leaf-tensor/21751/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = optim.SGD(memn2n.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 34])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8665, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(nll_loss, answers.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[layer-embedding_C_1] sum of papmerters: 3.1944\n",
      "[layer-embedding_A_2] sum of papmerters: 3.1944\n"
     ]
    }
   ],
   "source": [
    "check_parameters(memn2n, 'embedding_C_1')\n",
    "check_parameters(memn2n, 'embedding_A_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "TASK = 1\n",
    "BATCH = 32\n",
    "FIX_STORY = None\n",
    "\n",
    "EMBED = 30\n",
    "W_STYLE = 'adjacent'\n",
    "ENC_METHOD = 'bow'\n",
    "TEMPROAL = False\n",
    "STEP = 10\n",
    "LR = 0.01\n",
    "ANNEAL = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "babi = bAbI()\n",
    "train, valid, test = babi.splits(root='../data/QA_bAbI_tasks/en-valid-10k/', \n",
    "                                 task=TASK, \n",
    "                                 fix_maxlen_story=FIX_STORY,\n",
    "                                 device=DEVICE)\n",
    "train_loader, valid_loader, test_loader = babi.iters(train, valid, test, BATCH)\n",
    "VOCAB = train.vocab\n",
    "MAXLEN = train.maxlen_story\n",
    "\n",
    "memn2n = MEMN2N(vocab_size=len(VOCAB), embed_size=15, weight_style='rnnlike', \n",
    "                encoding_method='pe', temporal=True, maxlen_story=MAXLEN,\n",
    "                pad_idx=VOCAB.stoi['<pad>']).to(DEVICE)\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=VOCAB.stoi['<pad>'], reduction='sum')\n",
    "optimizer = optim.SGD(memn2n.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=ANNEAL, milestones=[25, 50, 75], \n",
    "                                           optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] loss: 60.7465, lr: 0.01,\n",
      "[2/10] loss: 56.6869, lr: 0.01,\n",
      "[3/10] loss: 44.6670, lr: 0.01,\n",
      "[4/10] loss: 7.4042, lr: 0.01,\n",
      "[5/10] loss: 0.0937, lr: 0.01,\n",
      "[6/10] loss: 0.0498, lr: 0.01,\n",
      "[7/10] loss: 0.0351, lr: 0.01,\n",
      "[8/10] loss: 0.0273, lr: 0.01,\n",
      "[9/10] loss: 0.0223, lr: 0.01,\n",
      "[10/10] loss: 0.0189, lr: 0.01,\n"
     ]
    }
   ],
   "source": [
    "for step in range(STEP):\n",
    "    \n",
    "    scheduler.step()\n",
    "    for b in train_loader:\n",
    "        stories, queries, answers = b\n",
    "        stories_idx = get_story_idx(stories, VOCAB.stoi['<pad>'])\n",
    "        memn2n.zero_grad()\n",
    "        \n",
    "        nll_loss = memn2n(stories, queries, stories_idx, ls=False)\n",
    "        loss = loss_function(nll_loss, answers.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(memn2n.parameters(), 50.0)  # gradient clipping\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    if step % 1 == 0:\n",
    "        string = '[{}/{}] loss: {:.4f}, lr: {},'.format(step+1, STEP, np.mean(losses), scheduler.get_lr()[0])\n",
    "        print(string)\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
